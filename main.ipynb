{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "# fetch input text\n",
    "import urllib.request\n",
    "url = 'https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt'\n",
    "file_path = 'the-verdict.txt'\n",
    "# urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "# split text into tokens\n",
    "import re\n",
    "preprocessed = [item for item in re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text) if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n",
      "('His', 51)\n"
     ]
    }
   ],
   "source": [
    "# convert tokens into token IDs\n",
    "\n",
    "# create vocabulary\n",
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 50:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {id:token for token,id in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = [token.strip() for token in re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) if token.strip()]\n",
    "        return [self.str_to_int[s] for s in preprocessed]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        # \"Hello , world !\"  →  \"Hello, world!\"\n",
    "        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', \" \".join([self.int_to_str[id] for id in ids]))\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = 'I turned to Mrs. Gisburn, who had lingered to give a lump of sugar to her spaniel in the dining-room.'\n",
    "print(tokenizer.decode(tokenizer.encode(text)) == text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# add special token for vocabulary\n",
    "all_tokens = all_words + ['<|endoftext|>', '<|unk|>']\n",
    "vocab_size = len(all_tokens)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    if i > len(vocab) - 5:\n",
    "        print(item)\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {id:token for token,id in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = [token.strip() for token in re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) if token.strip()]\n",
    "        preprocessed = [token if token in self.str_to_int else '<|unk|>' for token in preprocessed]\n",
    "        return [self.str_to_int[s] for s in preprocessed]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        # \"Hello , world !\"  →  \"Hello, world!\"\n",
    "        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', \" \".join([self.int_to_str[id] for id in ids]))\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n",
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n",
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n",
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n",
      "\n",
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "# data sampling with sliding window\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "# context size determines how many tokens are included int the input\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n",
    "\n",
    "# iterator all target\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    target = enc_sample[i]\n",
    "    print(context, \"---->\", target)\n",
    "print()\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    target = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([target]))\n",
    "\n",
    "# data loader\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        tokens_id = tokenizer.encode(text)\n",
    "        for i in range(0, len(tokens_id) - max_length, stride):\n",
    "            input_chunk = tokens_id[i:i+max_length]\n",
    "            output_chunk = tokens_id[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(output_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(text, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n",
      "\n",
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)\n",
    "print()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"Targets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# token embeddings example\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)\n",
    "print(embedding_layer(torch.tensor([3])))\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n",
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# encoding word position\n",
    "vocab_size = 50257\n",
    "embed_dim = 256\n",
    "# token embedding\n",
    "token_emb = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "# load batch data\n",
    "inputs, _ = next(iter(create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)))\n",
    "token_vecs = token_emb(inputs)\n",
    "print(token_vecs.shape)\n",
    "\n",
    "# position embedding, max_length is 4, so it has '4' position\n",
    "pos_emb = torch.nn.Embedding(4, embed_dim)\n",
    "pos_vecs = pos_emb(torch.arange(4))\n",
    "print(pos_vecs.shape)\n",
    "\n",
    "# gpt input\n",
    "input_embeds = token_vecs + pos_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "Attention weight: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n",
      "Attention weight: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n",
      "Context vec: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# self-attention example\n",
    "# 1.计算查询与每个输入之间的点积（注意力得分）；\n",
    "# 2.使用 softmax 将得分归一化为权重；\n",
    "# 3.按权重求输入向量的加权和，得到上下文向量。\n",
    "\n",
    "# this is the embedding inputs\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89],  # Your    (x¹)\n",
    "     [0.55, 0.87, 0.66],  # journey (x²)\n",
    "     [0.57, 0.85, 0.64],  # starts  (x³)\n",
    "     [0.22, 0.58, 0.33],  # with    (x⁴)\n",
    "     [0.77, 0.25, 0.10],  # one     (x⁵)\n",
    "     [0.05, 0.80, 0.55]]  # step    (x⁶)\n",
    ")\n",
    "# the first step is to calculate the **attention scores** between the query token and each input token\n",
    "# calculate the attention score of x2\n",
    "query = inputs[1]\n",
    "attention_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i,x_i in enumerate(inputs):\n",
    "    attention_scores_2[i] = torch.dot(query, x_i)\n",
    "print(attention_scores_2)\n",
    "# normalize the score\n",
    "attention_weights_2_tmp = attention_scores_2 / attention_scores_2.sum()\n",
    "print(\"Attention weight:\", attention_weights_2_tmp)\n",
    "print(\"Sum:\", attention_weights_2_tmp.sum())\n",
    "# use softmax to normalize\n",
    "atention_weights_2 = torch.softmax(attention_scores_2, dim=0)\n",
    "print(\"Attention weight:\", atention_weights_2)\n",
    "print(\"Sum:\", atention_weights_2.sum())\n",
    "# calculate the context vector\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += atention_weights_2[i] * x_i\n",
    "print(\"Context vec:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n",
      "Previous 2nd context vec: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# calculate the attention weights for all input tokens\n",
    "attention_scores = torch.empty(6, 6)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    for j,x_j in enumerate(inputs):\n",
    "        attention_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attention_scores)\n",
    "\n",
    "attention_scores = inputs @ inputs.T\n",
    "print(attention_scores)\n",
    "# normalize the scores\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "print(attention_weights.sum(dim=-1))\n",
    "# context vec\n",
    "all_context_vecs = attention_weights @ inputs\n",
    "print(all_context_vecs)\n",
    "print(\"Previous 2nd context vec:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## self-attention with trainable weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n",
      "torch.Size([3, 2])\n",
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "tensor(1.8524, grad_fn=<DotBackward0>)\n",
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "       grad_fn=<SqueezeBackward4>)\n",
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.3061, 0.8210], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# use Wq,Wk,Wv\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2 # for example\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out, requires_grad=False))\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out, requires_grad=False))\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)\n",
    "print(W_query.shape)\n",
    "# all inputs\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "# attention score for x2\n",
    "keys_2 = keys[1]\n",
    "attention_score22 = query_2.dot(keys_2)\n",
    "print(attention_score22)\n",
    "# all attention scores for give query 2\n",
    "attention_scores_2 = query_2 @ keys.T\n",
    "print(attention_scores_2)\n",
    "# scale\n",
    "d_k = keys.shape[-1]\n",
    "attention_weights_2 = torch.softmax(attention_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attention_weights_2)\n",
    "# context vec\n",
    "context_vec_2 = attention_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vecs = attention_weights @ values\n",
    "        return context_vecs\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5337, -0.1051],\n",
      "        [-0.5323, -0.1080],\n",
      "        [-0.5323, -0.1079],\n",
      "        [-0.5297, -0.1076],\n",
      "        [-0.5311, -0.1066],\n",
      "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# use nn.Linear\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vecs = attention_weights @ values\n",
    "        return context_vecs\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hide future words with causal attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n",
      "        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n",
      "        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n",
      "        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n",
      "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0.1717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1636, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1637, 0.1749, 0.1746, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1636, 0.1704, 0.1702, 0.1652, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1855, 0.8812],\n",
      "        [0.2938, 0.9445],\n",
      "        [0.3258, 0.9576],\n",
      "        [0.3036, 0.8564],\n",
      "        [0.2737, 0.7564],\n",
      "        [0.2818, 0.7599]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 注意力矩阵主对角线之上的「未来词元」对应的权重全部遮蔽（置为 -∞），然后仅对未被遮蔽的权重做 softmax 归一化，使得每一行有效权重之和为 1\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attention_scores = queries @ keys.T\n",
    "attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim= -1)\n",
    "print(attention_weights)\n",
    "# create a mask where the values above the diagonal are zero\n",
    "context_length = attention_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length), diagonal=0)\n",
    "print(mask_simple)\n",
    "mask_simple = attention_weights * mask_simple\n",
    "print(mask_simple)\n",
    "row_sums = mask_simple.sum(dim=-1, keepdim=True)\n",
    "mask_simple_normalize = mask_simple / row_sums\n",
    "# print(mask_simple_normalize)\n",
    "\n",
    "# another method to mask attention weights\n",
    "# 对角线以上全为1\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "# print(masked)\n",
    "attention_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attention_weights)\n",
    "context_vec = attention_weights @ values\n",
    "print(context_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n",
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.6816, 0.6804, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5085, 0.4936, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3906, 0.0000],\n",
      "        [0.3249, 0.3418, 0.0000, 0.3308, 0.3249, 0.3363]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "# Masking additional attention weights with dropout\n",
    "torch.manual_seed(123)\n",
    "drop_out = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(drop_out(example))\n",
    "print(drop_out(attention_weights))\n",
    "# test batch inputs\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, drop_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, _ = x.shape\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # here we think about the batch dim: ->(b, d_out, num_tokens)\n",
    "        attention_scores = queries @ keys.transpose(1, 2)\n",
    "        attention_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        context_vecs = attention_weights @ values\n",
    "        return context_vecs\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, drop_out=0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending single-head attention to multi-head attention\n",
    "\n",
    "### Stacking multiple single-head attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "# stack CausalAttention\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, drop_out, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CausalAttention(d_in, d_out, context_length, drop_out, qkv_bias) for _ in range(num_heads)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in,d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing multi-head attention with weight splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, drop_out, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisiable by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # why we need a output projection?\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.drop_out = nn.Dropout(drop_out)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x:(batch, num_tokens, d_in)\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # q:(batch, num_tokens, d_out)\n",
    "        q = self.W_query(x)\n",
    "        k = self.W_key(x)\n",
    "        v = self.W_value(x)\n",
    "\n",
    "\n",
    "        # view:(batch, heads, num_tokens, head_dim)\n",
    "        q = q.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "\n",
    "        # attention_scores:(batch, heads, num_tokens, num_tokens)\n",
    "        attention_scores = q @ k.transpose(2, 3)\n",
    "        attention_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "\n",
    "        attention_weights = torch.softmax(attention_scores / k.shape[-1]**0.5, dim=-1)\n",
    "        attention_weights = self.drop_out(attention_weights)\n",
    "\n",
    "        # context_vecs:(batch, num_tokens, heads, head_dim)\n",
    "        context_vecs = (attention_weights @ v).transpose(1, 2)\n",
    "        context_vecs = context_vecs.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vecs = self.out_proj(context_vecs)\n",
    "        return context_vecs\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_size, context_len, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_len, drop_out=0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT\n",
    "## LLM architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257, # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768, # Embedding dimension\n",
    "    \"n_heads\": 12, # Number of attention heads\n",
    "    \"n_layers\": 12, # Number of layers\n",
    "    \"drop_rate\": 0.1, # Dropout rate\n",
    "    \"qkv_bias\": False # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class DummyLinearNorm(nn.Module):\n",
    "    def __init__(self, normalized_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(cfg[\"vocab_sizes\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emd_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(*[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = DummyLinearNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        token_emb = self.token_emb(in_idx)\n",
    "        pos_emb = self.pos_emb(torch.arange(seq_len))\n",
    "        x = token_emb + pos_emb\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch = torch.stack([torch.tensor(tokenizer.encode(txt1)), torch.tensor(tokenizer.encode(txt2))], dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor(-4.7684e-08, grad_fn=<MeanBackward0>) tensor(1.1111, grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Layer Normalization\n",
    "\n",
    "torch.manual_seed(123)\n",
    "example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(example)\n",
    "# print(out)\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "# print(mean)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "# print(var)\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "print(out_norm)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "# print(mean)\n",
    "# print(var)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return self.scale * (x - mean) / (torch.sqrt(var + self.eps)) + self.shift\n",
    "    \n",
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(example)\n",
    "print(out_ln.mean(), out_ln.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward network with GELU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    # 将张量转成列表，避免调用 .numpy()\n",
    "    plt.plot(x.tolist(), y.tolist())\n",
    "    plt.title(f\"{label} activation\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "# (批, 令牌数, 768)\n",
    "#       │  Linear 768→3072\n",
    "#       ▼\n",
    "# (批, 令牌数, 3072)\n",
    "#       │  GELU\n",
    "#       ▼\n",
    "# (批, 令牌数, 3072)\n",
    "#       │  Linear 3072→768\n",
    "#       ▼\n",
    "# (批, 令牌数, 768)\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcut connections\n",
    "将输入再次加到该层的输出，缓解梯度衰减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without shortcut:\n",
      "layers.0.0.weight's grad mean: 0.0002\n",
      "layers.1.0.weight's grad mean: 0.0001\n",
      "layers.2.0.weight's grad mean: 0.0007\n",
      "layers.3.0.weight's grad mean: 0.0014\n",
      "layers.4.0.weight's grad mean: 0.0050\n",
      "With shortcut:\n",
      "layers.0.0.weight's grad mean: 0.0014\n",
      "layers.1.0.weight's grad mean: 0.0048\n",
      "layers.2.0.weight's grad mean: 0.0041\n",
      "layers.3.0.weight's grad mean: 0.0059\n",
      "layers.4.0.weight's grad mean: 0.0327\n"
     ]
    }
   ],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_size, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        # five \"Linear -> GELU\"\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_size[0], layer_size[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[1], layer_size[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[2], layer_size[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[3], layer_size[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[4], layer_size[5]), GELU()),\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()(output, target)\n",
    "    loss.backward()\n",
    "    for name, param in model.named_parameters():\n",
    "        # print(name) #`layers.0.0.bias` and `layers.1.0.weight`\n",
    "        # print(param.shape)\n",
    "        if \"weight\" in name:\n",
    "            g_mean = param.grad.abs().mean().item()\n",
    "            print(f\"{name}'s grad mean: {g_mean:.4f}\")\n",
    "\n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_no_short = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)\n",
    "print(\"Without shortcut:\")\n",
    "print_gradients(model_no_short, sample_input)\n",
    "\n",
    "model_no_short = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "print(\"With shortcut:\")\n",
    "print_gradients(model_no_short, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection attention and linear in a transformer block\n",
    "![connect_attention_linear](connect_attention_linear.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            drop_out=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        # Linear -> GELU -> Linear\n",
    "        self.ff = FeedForward(cfg)\n",
    "\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self attention\n",
    "        shortcut_x = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut_x\n",
    "\n",
    "        # feed forward\n",
    "        shortcut_x = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut_x\n",
    "        return x\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "out = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT model\n",
    "\n",
    "![GPT](gpt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "torch.return_types.max(\n",
      "values=tensor([[2.6121, 2.5646, 2.3975, 2.3694],\n",
      "        [2.3718, 2.9749, 2.7942, 2.4123]], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([[ 2215, 36230, 49103, 37532],\n",
      "        [40412, 30882, 46430, 29873]]))\n",
      "parameters: 163059793\n",
      "torch.Size([50257, 768])\n",
      "torch.Size([1024, 768])\n",
      "Number of trainable parameters: 124412160\n",
      "Total size of the model: 622.02 MB\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257, # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768, # Embedding dimension\n",
    "    \"n_heads\": 12, # Number of attention heads\n",
    "    \"n_layers\": 12, # Number of layers\n",
    "    \"drop_rate\": 0.1, # Dropout rate\n",
    "    \"qkv_bias\": False # Query-Key-Value bias\n",
    "}\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_out = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_layer_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.output_layer = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch, context_length = x.shape\n",
    "        token_emb = self.tok_emb(x)\n",
    "        pos_ids = torch.arange(context_length, device=x.device)\n",
    "        pos_emb = self.pos_emb(pos_ids)\n",
    "        \n",
    "        x = token_emb + pos_emb\n",
    "        x = self.drop_out(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_layer_norm(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out.max(dim=-1))\n",
    "total_params = sum([p.numel() for p in model.parameters()])\n",
    "print(\"parameters:\", total_params)\n",
    "print(model.tok_emb.weight.shape)\n",
    "print(model.pos_emb.weight.shape)\n",
    "\n",
    "total_params_gpt2 = total_params - sum([p.numel() for p in model.output_layer.parameters()])\n",
    "\n",
    "print(\"Number of trainable parameters:\", total_params_gpt2)\n",
    "\n",
    "total_size_bytes = total_params * 4        # 32-bit float ⇒ 4 B\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text\n",
    "\n",
    "![generation_mechanics](generation_mechanics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 仅保留最近 context_size 个 token 作为上下文，防止超过最大上下文长度\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "            # 每个batch的最后一行 (batch,vocab_size)\n",
    "            logits = logits[:, -1, :] \n",
    "        \n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![iteration](iteration.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length, 10\n",
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(model=model, idx=encoded_tensor, max_new_tokens=6, context_size=GPT_CONFIG_124M[\"context_length\"])\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length,\", len(out[0]))\n",
    "docoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(docoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining on unlabeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      " Every effort moves you rentingetic chief refusing holidays Shannon GamergateHay men methamphetamine\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,   # 将上下文长度缩短至 256\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(model=model, idx=text_to_token_ids(start_context, tokenizer), max_new_tokens=10, context_size=GPT_CONFIG_124M[\"context_length\"])\n",
    "print(\"Result:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the text generation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n",
      "Token IDs:\n",
      " tensor([[[50153],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n",
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  PRESIDENT heNetflix\n",
      "Text 1: tensor([7.6198e-05, 3.1919e-05, 1.1728e-05])\n",
      "Text 2: tensor([1.0538e-05, 5.5378e-05, 4.9063e-06])\n",
      "log_probas: tensor([ -9.4822, -10.3523, -11.3535, -11.4605,  -9.8013, -12.2250])\n",
      "tensor(-10.7791)\n",
      "tensor(10.7791)\n",
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n",
      "tensor(10.7791)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([\n",
    "    [16833,  3626,  6100],   # \"every effort moves\"\n",
    "    [   40,  1107,   588]    # \"I really like\"\n",
    "])\n",
    "targets = torch.tensor([\n",
    "    [ 3626,  6100,   345],   # \" effort moves you\"\n",
    "    [ 1107,   588, 11311]    # \" really like chocolate\"\n",
    "])\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs) #[2, 3, 50257]\n",
    "    probas = torch.softmax(logits, dim=-1)\n",
    "    print(probas.shape)\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n",
    "\n",
    "# 目标 token 对应的初始 softmax 概率分数,我们的目的是最大化target_probas\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)\n",
    "\n",
    "# loss 在数学优化中，直接处理概率分数不如处理其对数方便\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(\"log_probas:\", log_probas)\n",
    "# 训练的目标是通过更新模型权重，使平均对数概率尽可能逼近 0。不过在深度学习实践中，我们通常不是直接把平均对数概率推高到 0，而是最小化其相反数（负平均对数概率）\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)\n",
    "# 将 −10.7940 取负得到 10.7940 的做法被称为“交叉熵损失”（cross entropy loss）\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape) # [2, 3, 50257]\n",
    "print(\"Targets shape:\", targets.shape) # [2, 3]\n",
    "\n",
    "# flatten in batch dim for cross_entropy\n",
    "logits_flat = logits.flatten(0, 1) # [batch * seq_len, vocab_size]\n",
    "targets_flat = targets.flatten()\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the training and validation set losses\n",
    "\n",
    "![prepare_data](prepare_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n",
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)\n",
    "\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.990697224934896\n",
      "Validation loss: 10.985529899597168\n"
     ]
    }
   ],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    # (batch*seq_len, vocab_size) (batch*seq_len)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches = None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches == None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\",   train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "![training_iteration](training_iteration.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20, step 000179 train_loss4.839, val_loss6.967: 100%|██████████| 20/20 [00:22<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "def evalute_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        # 在start_context后面输出50个token\n",
    "        token_ids = generate_text_simple(model, idx=encoded, max_new_tokens=100, context_size=context_size)\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "def train_model_sample(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    token_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in (t := trange(num_epochs)):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            token_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evalute_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(token_seen)\n",
    "\n",
    "            t.set_description(f\"Ep {epoch + 1}, step {global_step:06d} train_loss{train_loss:.3f}, val_loss{val_loss:.3f}\")\n",
    "\n",
    "        # generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    \n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses, val_losses, tokens_seen = train_model_sample(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq=5, eval_iter=5,\n",
    "                                                       start_context=\"Every effort moves you\", tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you---- of the of the of the                       of the I had, and the first.     of thehum.                                                     \n"
     ]
    }
   ],
   "source": [
    "generate_and_print_sample(model, tokenizer, device, \"Every effort moves you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Train loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label = \"Val loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))  # 生成均匀分布的 epoch 值\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you---- of the of the of the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(model, idx=text_to_token_ids(\"Every effort moves you\", tokenizer), max_new_tokens=25, context_size=GPT_CONFIG_124M[\"context_length\"])\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(358.5170, grad_fn=<SumBackward0>)\n",
      "tensor(-517.4147, grad_fn=<SumBackward0>)\n",
      "tensor(354.6163, grad_fn=<SumBackward0>)\n",
      "tensor(2835.3945, grad_fn=<SumBackward0>)\n",
      "tensor(3137.7725, grad_fn=<SumBackward0>)\n",
      "tensor(3850.1240, grad_fn=<SumBackward0>)\n",
      "tensor(4550.8623, grad_fn=<SumBackward0>)\n",
      "tensor(5560.6016, grad_fn=<SumBackward0>)\n",
      "tensor(6201.0537, grad_fn=<SumBackward0>)\n",
      "tensor(7119.9932, grad_fn=<SumBackward0>)\n",
      "tensor(9122.9141, grad_fn=<SumBackward0>)\n",
      "tensor(-5792.8228, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import safetensors\n",
    "import struct\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "f = open(\"data\", \"rb\")\n",
    "token_bytes = f.read(65 * 2)\n",
    "tokens = struct.unpack(\"65H\", token_bytes)\n",
    "# print(tokens)\n",
    "# print(len(tokens))\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt2\")\n",
    "# print(enc.decode(list(tokens[:65])))\n",
    "\n",
    "f = open(\"model.safetensors\", \"rb\")\n",
    "fts = safetensors.deserialize(f.read())\n",
    "params = dict()\n",
    "for ft in fts:\n",
    "    name = ft[0]\n",
    "    data = ft[1][\"data\"]\n",
    "    shape = ft[1][\"shape\"]\n",
    "    params[name] = torch.frombuffer(data, dtype=torch.float32, requires_grad=True).reshape(shape)\n",
    "    params[name].retain_grad()\n",
    "\n",
    "x = torch.tensor(tokens[:64], dtype=torch.long)\n",
    "y_true = torch.tensor(tokens[1:65], dtype=torch.long)\n",
    "\n",
    "input_size = len(x)\n",
    "d_model = 768\n",
    "d_k = 64\n",
    "\n",
    "wte_out = F.embedding(\n",
    "    input=x,\n",
    "    weight=params[\"wte.weight\"])\n",
    "wpe_out = F.embedding(\n",
    "    input=torch.arange(0, len(x), dtype=torch.long),\n",
    "    weight=params[\"wpe.weight\"])\n",
    "embedding_out = wte_out + wpe_out\n",
    "# print(embedding_out.sum())\n",
    "for layer_i in range(12):\n",
    "    layer_in = embedding_out if layer_i == 0 else res_2_out\n",
    "    ln_1_out = F.layer_norm(\n",
    "        input=layer_in,\n",
    "        normalized_shape=[d_model],\n",
    "        weight=params[f\"h.{layer_i}.ln_1.weight\"],\n",
    "        bias=params[f\"h.{layer_i}.ln_1.bias\"],\n",
    "        eps=1e-5)\n",
    "    # print(ln_1_out.sum())\n",
    "    attn_c_attn_out = F.linear(\n",
    "        input=ln_1_out,\n",
    "        weight=params[f\"h.{layer_i}.attn.c_attn.weight\"].transpose(0, 1),\n",
    "        bias=params[f\"h.{layer_i}.attn.c_attn.bias\"])\n",
    "    # print(attn_c_attn_out.sum())\n",
    "    q, k, v = attn_c_attn_out.split(d_model, dim=1)\n",
    "    attn_z_out = torch.zeros([input_size, d_model])\n",
    "    for head_i in range(12):\n",
    "        a = q[:,head_i*d_k:(head_i+1)*d_k] @ k[:,head_i*d_k:(head_i+1)*d_k].transpose(0, 1) * torch.rsqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "        mask = torch.triu(torch.ones_like(a, dtype=torch.bool), diagonal=1)\n",
    "        a = torch.masked_fill(a, mask, -torch.inf)\n",
    "        s = F.softmax(a, dim=-1)\n",
    "        \n",
    "        z = s @ v[:,head_i*d_k:(head_i+1)*d_k]\n",
    "        attn_z_out[:,head_i*d_k:(head_i+1)*d_k] = z\n",
    "    \n",
    "    # print(attn_z_out.sum())\n",
    "    # exit()\n",
    "    attn_c_proj_out = F.linear(\n",
    "        input=attn_z_out,\n",
    "        weight=params[f\"h.{layer_i}.attn.c_proj.weight\"].transpose(0, 1),\n",
    "        bias=params[f\"h.{layer_i}.attn.c_proj.bias\"])\n",
    "    # print(attn_c_proj_out.sum())\n",
    "    # exit()\n",
    "    res_1_out = layer_in + attn_c_proj_out\n",
    "    # print(res_1_out.sum())\n",
    "    # exit()\n",
    "    ln_2_out = F.layer_norm(\n",
    "        input=res_1_out,\n",
    "        normalized_shape=[d_model],\n",
    "        weight=params[f\"h.{layer_i}.ln_2.weight\"],\n",
    "        bias=params[f\"h.{layer_i}.ln_2.bias\"],\n",
    "        eps=1e-5)\n",
    "    mlp_c_fc_out = F.linear(\n",
    "        input=ln_2_out,\n",
    "        weight=params[f\"h.{layer_i}.mlp.c_fc.weight\"].transpose(0, 1),\n",
    "        bias=params[f\"h.{layer_i}.mlp.c_fc.bias\"])\n",
    "    mlp_gelu_out = F.gelu(mlp_c_fc_out)\n",
    "    mlp_c_proj_out = F.linear(\n",
    "        input=mlp_gelu_out,\n",
    "        weight=params[f\"h.{layer_i}.mlp.c_proj.weight\"].transpose(0, 1),\n",
    "        bias=params[f\"h.{layer_i}.mlp.c_proj.bias\"])\n",
    "    res_2_out = res_1_out + mlp_c_proj_out\n",
    "\n",
    "ln_f_out = F.layer_norm(\n",
    "    input=res_2_out,\n",
    "    normalized_shape=[d_model],\n",
    "    weight=params[\"ln_f.weight\"],\n",
    "    bias=params[\"ln_f.bias\"],\n",
    "    eps=1e-5)\n",
    "\n",
    "y_pred = F.linear(\n",
    "    input=ln_f_out,\n",
    "    weight=params[\"wte.weight\"])\n",
    "\n",
    "loss = F.cross_entropy(y_pred, y_true)\n",
    "assert(abs(loss - 4.1333) < 1e-3)\n",
    "\n",
    "# loss.backward()\n",
    "# print(params[\"wte.weight\"].grad.sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

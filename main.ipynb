{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 16384\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "# fetch input text\n",
    "import urllib.request\n",
    "url = 'https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt'\n",
    "file_path = 'the-verdict.txt'\n",
    "# urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3702\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "# split text into tokens\n",
    "import re\n",
    "preprocessed = [item for item in re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text) if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1013\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Arrt', 15)\n",
      "('As', 16)\n",
      "('Be', 17)\n",
      "('Burlington', 18)\n",
      "('But', 19)\n",
      "('By', 20)\n",
      "('Carlo', 21)\n",
      "('Chicago', 22)\n",
      "('Claude', 23)\n",
      "('Come', 24)\n",
      "('Croft', 25)\n",
      "('Destroyed', 26)\n",
      "('Don', 27)\n",
      "('Dubarry', 28)\n",
      "('Emperors', 29)\n",
      "('Florence', 30)\n",
      "('For', 31)\n",
      "('Gallery', 32)\n",
      "('Gideon', 33)\n",
      "('Gisburn', 34)\n",
      "('Gisburns', 35)\n",
      "('Grafton', 36)\n",
      "('Grindle', 37)\n",
      "('Grindles', 38)\n",
      "('HAD', 39)\n",
      "('Had', 40)\n",
      "('Has', 41)\n",
      "('He', 42)\n",
      "('Her', 43)\n",
      "('Hermia', 44)\n",
      "('His', 45)\n",
      "('How', 46)\n",
      "('I', 47)\n",
      "('If', 48)\n",
      "('In', 49)\n",
      "('It', 50)\n",
      "('Jack', 51)\n"
     ]
    }
   ],
   "source": [
    "# convert tokens into token IDs\n",
    "\n",
    "# create vocabulary\n",
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 50:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {id:token for token,id in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = [token.strip() for token in re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) if token.strip()]\n",
    "        return [self.str_to_int[s] for s in preprocessed]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        # \"Hello , world !\"  →  \"Hello, world!\"\n",
    "        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', \" \".join([self.int_to_str[id] for id in ids]))\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = 'I turned to Mrs. Gisburn, who had lingered to give a lump of sugar to her spaniel in the dining-room.'\n",
    "print(tokenizer.decode(tokenizer.encode(text)) == text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1015\n",
      "('your', 1011)\n",
      "('yourself', 1012)\n",
      "('<|endoftext|>', 1013)\n",
      "('<|unk|>', 1014)\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1014, 5, 316, 1009, 562, 873, 10, 1013, 49, 883, 857, 880, 649, 883, 1014, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# add special token for vocabulary\n",
    "all_tokens = all_words + ['<|endoftext|>', '<|unk|>']\n",
    "vocab_size = len(all_tokens)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    if i > len(vocab) - 5:\n",
    "        print(item)\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {id:token for token,id in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = [token.strip() for token in re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) if token.strip()]\n",
    "        preprocessed = [token if token in self.str_to_int else '<|unk|>' for token in preprocessed]\n",
    "        return [self.str_to_int[s] for s in preprocessed]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        # \"Hello , world !\"  →  \"Hello, world!\"\n",
    "        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', \" \".join([self.int_to_str[id] for id in ids]))\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n",
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4114\n",
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n",
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n",
      "\n",
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "# data sampling with sliding window\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "# context size determines how many tokens are included int the input\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n",
    "\n",
    "# iterator all target\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    target = enc_sample[i]\n",
    "    print(context, \"---->\", target)\n",
    "print()\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    target = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([target]))\n",
    "\n",
    "# data loader\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        tokens_id = tokenizer.encode(text)\n",
    "        for i in range(0, len(tokens_id) - max_length, stride):\n",
    "            input_chunk = tokens_id[i:i+max_length]\n",
    "            output_chunk = tokens_id[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(output_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(text, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n",
      "\n",
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)\n",
    "print()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"Targets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# token embeddings example\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)\n",
    "print(embedding_layer(torch.tensor([3])))\n",
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n",
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# encoding word position\n",
    "vocab_size = 50257\n",
    "embed_dim = 256\n",
    "# token embedding\n",
    "token_emb = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "# load batch data\n",
    "inputs, _ = next(iter(create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)))\n",
    "token_vecs = token_emb(inputs)\n",
    "print(token_vecs.shape)\n",
    "\n",
    "# position embedding, max_length is 4, so it has '4' position\n",
    "pos_emb = torch.nn.Embedding(4, embed_dim)\n",
    "pos_vecs = pos_emb(torch.arange(4))\n",
    "print(pos_vecs.shape)\n",
    "\n",
    "# gpt input\n",
    "input_embeds = token_vecs + pos_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "Attention weight: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n",
      "Attention weight: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n",
      "Context vec: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# self-attention example\n",
    "# 1.计算查询与每个输入之间的点积（注意力得分）；\n",
    "# 2.使用 softmax 将得分归一化为权重；\n",
    "# 3.按权重求输入向量的加权和，得到上下文向量。\n",
    "\n",
    "# this is the embedding inputs\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89],  # Your    (x¹)\n",
    "     [0.55, 0.87, 0.66],  # journey (x²)\n",
    "     [0.57, 0.85, 0.64],  # starts  (x³)\n",
    "     [0.22, 0.58, 0.33],  # with    (x⁴)\n",
    "     [0.77, 0.25, 0.10],  # one     (x⁵)\n",
    "     [0.05, 0.80, 0.55]]  # step    (x⁶)\n",
    ")\n",
    "# the first step is to calculate the **attention scores** between the query token and each input token\n",
    "# calculate the attention score of x2\n",
    "query = inputs[1]\n",
    "attention_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i,x_i in enumerate(inputs):\n",
    "    attention_scores_2[i] = torch.dot(query, x_i)\n",
    "print(attention_scores_2)\n",
    "# normalize the score\n",
    "attention_weights_2_tmp = attention_scores_2 / attention_scores_2.sum()\n",
    "print(\"Attention weight:\", attention_weights_2_tmp)\n",
    "print(\"Sum:\", attention_weights_2_tmp.sum())\n",
    "# use softmax to normalize\n",
    "atention_weights_2 = torch.softmax(attention_scores_2, dim=0)\n",
    "print(\"Attention weight:\", atention_weights_2)\n",
    "print(\"Sum:\", atention_weights_2.sum())\n",
    "# calculate the context vector\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += atention_weights_2[i] * x_i\n",
    "print(\"Context vec:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n",
      "Previous 2nd context vec: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# calculate the attention weights for all input tokens\n",
    "attention_scores = torch.empty(6, 6)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    for j,x_j in enumerate(inputs):\n",
    "        attention_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attention_scores)\n",
    "\n",
    "attention_scores = inputs @ inputs.T\n",
    "print(attention_scores)\n",
    "# normalize the scores\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "print(attention_weights.sum(dim=-1))\n",
    "# context vec\n",
    "all_context_vecs = attention_weights @ inputs\n",
    "print(all_context_vecs)\n",
    "print(\"Previous 2nd context vec:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## self-attention with trainable weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n",
      "torch.Size([3, 2])\n",
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "tensor(1.8524, grad_fn=<DotBackward0>)\n",
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "       grad_fn=<SqueezeBackward4>)\n",
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.3061, 0.8210], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# use Wq,Wk,Wv\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2 # for example\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out, requires_grad=False))\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out, requires_grad=False))\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)\n",
    "print(W_query.shape)\n",
    "# all inputs\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "# attention score for x2\n",
    "keys_2 = keys[1]\n",
    "attention_score22 = query_2.dot(keys_2)\n",
    "print(attention_score22)\n",
    "# all attention scores for give query 2\n",
    "attention_scores_2 = query_2 @ keys.T\n",
    "print(attention_scores_2)\n",
    "# scale\n",
    "d_k = keys.shape[-1]\n",
    "attention_weights_2 = torch.softmax(attention_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attention_weights_2)\n",
    "# context vec\n",
    "context_vec_2 = attention_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vecs = attention_weights @ values\n",
    "        return context_vecs\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5337, -0.1051],\n",
      "        [-0.5323, -0.1080],\n",
      "        [-0.5323, -0.1079],\n",
      "        [-0.5297, -0.1076],\n",
      "        [-0.5311, -0.1066],\n",
      "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# use nn.Linear\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vecs = attention_weights @ values\n",
    "        return context_vecs\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hide future words with causal attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n",
      "        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n",
      "        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n",
      "        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n",
      "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0.1717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1636, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1637, 0.1749, 0.1746, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1636, 0.1704, 0.1702, 0.1652, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1855, 0.8812],\n",
      "        [0.2938, 0.9445],\n",
      "        [0.3258, 0.9576],\n",
      "        [0.3036, 0.8564],\n",
      "        [0.2737, 0.7564],\n",
      "        [0.2818, 0.7599]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 注意力矩阵主对角线之上的「未来词元」对应的权重全部遮蔽（置为 -∞），然后仅对未被遮蔽的权重做 softmax 归一化，使得每一行有效权重之和为 1\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attention_scores = queries @ keys.T\n",
    "attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim= -1)\n",
    "print(attention_weights)\n",
    "# create a mask where the values above the diagonal are zero\n",
    "context_length = attention_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length), diagonal=0)\n",
    "print(mask_simple)\n",
    "mask_simple = attention_weights * mask_simple\n",
    "print(mask_simple)\n",
    "row_sums = mask_simple.sum(dim=-1, keepdim=True)\n",
    "mask_simple_normalize = mask_simple / row_sums\n",
    "# print(mask_simple_normalize)\n",
    "\n",
    "# another method to mask attention weights\n",
    "# 对角线以上全为1\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "# print(masked)\n",
    "attention_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attention_weights)\n",
    "context_vec = attention_weights @ values\n",
    "print(context_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n",
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.6816, 0.6804, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5085, 0.4936, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3906, 0.0000],\n",
      "        [0.3249, 0.3418, 0.0000, 0.3308, 0.3249, 0.3363]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "# Masking additional attention weights with dropout\n",
    "torch.manual_seed(123)\n",
    "drop_out = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(drop_out(example))\n",
    "print(drop_out(attention_weights))\n",
    "# test batch inputs\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, drop_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, _ = x.shape\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # here we think about the batch dim: ->(b, d_out, num_tokens)\n",
    "        attention_scores = queries @ keys.transpose(1, 2)\n",
    "        attention_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        context_vecs = attention_weights @ values\n",
    "        return context_vecs\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, drop_out=0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending single-head attention to multi-head attention\n",
    "\n",
    "### Stacking multiple single-head attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "# stack CausalAttention\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, drop_out, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CausalAttention(d_in, d_out, context_length, drop_out, qkv_bias) for _ in range(num_heads)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in,d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing multi-head attention with weight splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m context_vecs\n\u001b[32m     48\u001b[39m torch.manual_seed(\u001b[32m123\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m batch_size, context_len, d_in = batch.shape\n\u001b[32m     50\u001b[39m d_out = \u001b[32m2\u001b[39m\n\u001b[32m     51\u001b[39m mha = MultiHeadAttention(d_in, d_out, context_len, drop_out=\u001b[32m0.0\u001b[39m, num_heads=\u001b[32m2\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, drop_out, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisiable by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # why we need a output projection?\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.drop_out = nn.Dropout(drop_out)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x:(batch, num_tokens, d_in)\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # q:(batch, num_tokens, d_out)\n",
    "        q = self.W_query(x)\n",
    "        k = self.W_key(x)\n",
    "        v = self.W_value(x)\n",
    "\n",
    "\n",
    "        # view:(batch, heads, num_tokens, head_dim)\n",
    "        q = q.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "\n",
    "        # attention_scores:(batch, heads, num_tokens, num_tokens)\n",
    "        attention_scores = q @ k.transpose(2, 3)\n",
    "        attention_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "\n",
    "        attention_weights = torch.softmax(attention_scores / k.shape[-1]**0.5, dim=-1)\n",
    "        attention_weights = self.drop_out(attention_weights)\n",
    "\n",
    "        # context_vecs:(batch, num_tokens, heads, head_dim)\n",
    "        context_vecs = (attention_weights @ v).transpose(1, 2)\n",
    "        context_vecs = context_vecs.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vecs = self.out_proj(context_vecs)\n",
    "        return context_vecs\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_size, context_len, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_len, drop_out=0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT\n",
    "## LLM architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257, # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768, # Embedding dimension\n",
    "    \"n_heads\": 12, # Number of attention heads\n",
    "    \"n_layers\": 12, # Number of layers\n",
    "    \"drop_rate\": 0.1, # Dropout rate\n",
    "    \"qkv_bias\": False # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class DummyLinearNorm(nn.Module):\n",
    "    def __init__(self, normalized_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(cfg[\"vocab_sizes\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emd_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(*[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = DummyLinearNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "    \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        token_emb = self.token_emb(in_idx)\n",
    "        pos_emb = self.pos_emb(torch.arange(seq_len))\n",
    "        x = token_emb + pos_emb\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch = torch.stack([torch.tensor(tokenizer.encode(txt1)), torch.tensor(tokenizer.encode(txt2))], dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor(-4.7684e-08, grad_fn=<MeanBackward0>) tensor(1.1111, grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Layer Normalization\n",
    "\n",
    "torch.manual_seed(123)\n",
    "example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(example)\n",
    "# print(out)\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "# print(mean)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "# print(var)\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "print(out_norm)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "# print(mean)\n",
    "# print(var)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return self.scale * (x - mean) / (torch.sqrt(var + self.eps)) + self.shift\n",
    "    \n",
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(example)\n",
    "print(out_ln.mean(), out_ln.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward network with GELU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAEiCAYAAABkykQ1AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZI1JREFUeJzt3XlYVGX7B/DvDMuwo4iAAiJuKC6okAbmVipuFZVki3taGvZmmiX+SlPfpDK33M2UJM0tlzIz0ST1dQdxh1xQVHZZZRmGmfP7A5lEQBm2c2b4fq6Lq+bMOXPum6l5uOc593NkgiAIICIiIiIiqga52AEQEREREZH+Y2FBRERERETVxsKCiIiIiIiqjYUFERERERFVGwsLIiIiIiKqNhYWRERERERUbSwsiIiIiIio2lhYEBERERFRtbGwICIiIiKiamNhQaQnvvjiC8hkMlHOHRoaCplMhlu3bolyfiIiKt+YMWPQvHlzUc4t5rhE0sTCgiQhLi4OkydPRps2bWBhYQELCwt4enoiKCgIFy5cKLVvyQdZRT9JSUkAgFu3bkEmk+Hbb7+t8LzNmzfH0KFDy33u7NmzkMlkCA0NrbE8nyYvLw9ffPEFIiIi6uycj5o/fz52794tyrmJiB5X8qVGyY+xsTGcnZ0xZswY3Lt3r0qvGRERAZlMhh07dlS4j0wmw+TJk8t9bseOHZDJZHX6OZ2QkIAvvvgC0dHRdXbOEmKPS6RfjMUOgGjv3r0YPnw4jI2N8fbbb8PLywtyuRwxMTHYuXMnVq1ahbi4OLi5uZU6btWqVbCysirzeg0aNKijyGteXl4e5syZAwDo06dPqec+++wzzJgxo1bPP3/+fAwbNgwBAQGlto8cORJvvPEGFApFrZ6fiKg8c+fOhbu7OwoKCnDy5EmEhobi2LFjuHTpEszMzMQOr9YlJCRgzpw5aN68OTp37lzque+//x4ajabWzi32uET6hYUFierGjRt444034ObmhkOHDqFJkyalnv/666+xcuVKyOVlJ9eGDRsGe3v7ugpVdMbGxjA2Fud/WSMjIxgZGYlybiKiQYMGwcfHBwAwfvx42Nvb4+uvv8avv/6K119/XeToxGViYiLaucUcl0iaeCkUieqbb75Bbm4uNmzYUKaoAIo/tP7zn//A1dVVhOgqJz09HR9//DE6duwIKysr2NjYYNCgQTh//nyZfQsKCvDFF1+gTZs2MDMzQ5MmTfDqq6/ixo0buHXrFho3bgwAmDNnjnbq/4svvgBQ9lrWDh06oG/fvmXOodFo4OzsjGHDhmm3ffvtt/Dz80OjRo1gbm4Ob2/vMpcByGQy5Obm4scff9See8yYMQAq7rFYuXIl2rdvD4VCgaZNmyIoKAiZmZml9unTpw86dOiAK1euoG/fvrCwsICzszO++eabyv6KiYhK6dmzJ4DiL6ceFRMTg2HDhsHOzg5mZmbw8fHBr7/+KkaIuH37Nt5//314eHjA3NwcjRo1QmBgYLm9apmZmfjoo4/QvHlzKBQKuLi4YNSoUUhLS0NERASeeeYZAMDYsWO1n88ll+k+2mOhUqlgZ2eHsWPHljlHdnY2zMzM8PHHHwMACgsLMWvWLHh7e8PW1haWlpbo2bMnDh8+rD1G13EJAIqKijBv3jy0bNkSCoUCzZs3x8yZM6FUKkvtV3Ip8rFjx9CtWzeYmZmhRYsW2Lhxo86/a5IOFhYkqr1796JVq1bo3r27zsemp6cjLS2t1M/jf9TWhZs3b2L37t0YOnQoFi1ahOnTp+PixYvo3bs3EhIStPup1WoMHToUc+bMgbe3NxYuXIgPP/wQWVlZuHTpEho3boxVq1YBAF555RWEhYUhLCwMr776arnnHT58OI4cOaLtKSlx7NgxJCQk4I033tBuW7p0Kbp06YK5c+di/vz5MDY2RmBgIH7//XftPmFhYVAoFOjZs6f23O+9916FeX/xxRcICgpC06ZNsXDhQrz22mtYs2YNBgwYAJVKVWrfjIwMDBw4EF5eXli4cCHatm2LTz/9FH/88Uflf9FERA+V/HHesGFD7bbLly/j2WefxdWrVzFjxgwsXLgQlpaWCAgIwK5du+o8xjNnzuD48eN444038N1332HixIk4dOgQ+vTpg7y8PO1+Dx48QM+ePbFs2TIMGDAAS5cuxcSJExETE4O7d++iXbt2mDt3LgDg3Xff1X4+9+rVq8w5TUxM8Morr2D37t0oLCws9dzu3buhVCq1Y0N2djbWrVuHPn364Ouvv8YXX3yB1NRU+Pv7a3s5dB2XgOIZpVmzZqFr165YvHgxevfujZCQkFJjUonr169j2LBh6N+/PxYuXIiGDRtizJgxuHz5sm6/bJIOgUgkWVlZAgAhICCgzHMZGRlCamqq9icvL0/73OzZswUA5f54eHho94uLixMACAsWLKgwBjc3N2HIkCHlPnfmzBkBgLBhw4Yn5lFQUCCo1epS2+Li4gSFQiHMnTtXu239+vUCAGHRokVlXkOj0QiCIAipqakCAGH27Nll9inJu0RsbKwAQFi2bFmp/d5//33Bysqq1O/s0X8XBEEoLCwUOnToIDz//POltltaWgqjR48uc+4NGzYIAIS4uDhBEAQhJSVFMDU1FQYMGFAq9+XLlwsAhPXr12u39e7dWwAgbNy4UbtNqVQKTk5OwmuvvVbmXEREJUo+ew4ePCikpqYKd+7cEXbs2CE0btxYUCgUwp07d7T7vvDCC0LHjh2FgoIC7TaNRiP4+fkJrVu31m47fPiwAEDYvn17hecFIAQFBZX73Pbt2wUAwuHDh58Y++Ofu4IgCCdOnCjzeThr1iwBgLBz584y+5eMDU8aj0aPHi24ublpH//5558CAOG3334rtd/gwYOFFi1aaB8XFRUJSqWy1D4ZGRmCo6OjMG7cOO02Xcal6OhoAYAwfvz4Uvt9/PHHAgDhr7/+0m5zc3MTAAhHjhzRbktJSREUCoUwbdq0Muci/cAZCxJNdnY2AJTbgN2nTx80btxY+7NixYoy+/zyyy8IDw8v9bNhw4Zaj/txCoVC2wOiVqtx//59WFlZwcPDA1FRUaXitbe3xwcffFDmNaqyXF+bNm3QuXNnbN26VbtNrVZjx44dePHFF2Fubq7d/ui/Z2RkICsrCz179iwVny4OHjyIwsJCTJkypVT/y4QJE2BjY1NqJgQofo9HjBihfWxqaopu3brh5s2bVTo/EdUv/fr1Q+PGjeHq6ophw4bB0tISv/76K1xcXAAUz2D/9ddfeP3115GTk6Odxb5//z78/f1x7dq1Kq8iVVWPfu6qVCrcv38frVq1QoMGDcqMDV5eXnjllVfKvEZVxobnn38e9vb2pcaGjIwMhIeHY/jw4dptRkZGMDU1BVB8CW16ejqKiorg4+NT5bFh3759AICpU6eW2j5t2jQAKDM2eHp6ai9rA4pnSDw8PDg26DF23JBorK2tARRPAz9uzZo1yMnJQXJycqk/SB/Vq1evOmneftoHu0ajwdKlS7Fy5UrExcVBrVZrn2vUqJH232/cuAEPD48abXQbPnw4Zs6ciXv37sHZ2RkRERFISUkpNXgAxZec/fe//0V0dHSp61yruv747du3AQAeHh6ltpuamqJFixba50u4uLiUOVfDhg3LLCVMRFSeFStWoE2bNsjKysL69etx5MiRUqvUXb9+HYIg4PPPP8fnn39e7mukpKTA2dm5xmJ62udnfn4+QkJCsGHDBty7dw+CIGify8rK0v77jRs38Nprr9VYXMbGxnjttdewefNmKJVKKBQK7Ny5EyqVqszY8OOPP2LhwoWIiYkpdQmru7t7lc59+/ZtyOVytGrVqtR2JycnNGjQoMzY0KxZszKv0bBhQ2RkZFTp/CQ+FhYkGltbWzRp0gSXLl0q81xJz0Vt35DNzMwM+fn55T5Xcg3s05YynD9/Pj7//HOMGzcO8+bNg52dHeRyOaZMmVKrSwACxYVFcHAwtm/fjilTpmDbtm2wtbXFwIEDtfscPXoUL730Enr16oWVK1eiSZMmMDExwYYNG7B58+Zaja9ERStKPTrQEhFVpFu3btpVoQICAvDcc8/hrbfeQmxsLKysrLSftR9//DH8/f3LfY3H/9h9EoVCUe2x4YMPPsCGDRswZcoU+Pr6wtbWFjKZDG+88Uatjw1vvPEG1qxZgz/++AMBAQHYtm0b2rZtCy8vL+0+P/30E8aMGYOAgABMnz4dDg4OMDIyQkhISJmmeF1V9ksrjg2Gh4UFiWrIkCFYt24dTp8+jW7dutX5+d3c3HDlypVyn4uNjdXu8yQ7duxA37598cMPP5TanpmZWWpGpWXLljh16hRUKlWFywPqOoPg7u6Obt26YevWrZg8eTJ27tyJgICAUt/k/fLLLzAzM8Off/5Zant5l41V9vwlv5PY2Fi0aNFCu72wsBBxcXHo16+fTnkQEVVWyR+/ffv2xfLlyzFjxgzt55CJiUmNfP64ublpx4DH6TI2jB49GgsXLtRuKygoKLPISMuWLcv9gu1Ruo4NvXr1QpMmTbB161Y899xz+Ouvv/B///d/ZeJr0aIFdu7cWer1Z8+eXeVzu7m5QaPR4Nq1a2jXrp12e3JyMjIzM5/6OyP9xx4LEtUnn3wCCwsLjBs3DsnJyWWer+1vLQYPHoy7d++Wudu0UqnEunXr4ODggK5duz7xNYyMjMrEuX379jLX87722mtIS0vD8uXLy7xGyfEWFhYAoNPqVsOHD8fJkyexfv16pKWllZnqNjIygkwmK3WJ1q1bt8q9w7alpWWlzt2vXz+Ympriu+++K5X7Dz/8gKysLAwZMqTS8RMR6apPnz7o1q0blixZgoKCAjg4OKBPnz5Ys2YNEhMTy+yfmpqq0+sPHjwYJ0+eRGRkZKntmZmZ2LRpEzp37gwnJ6cnvkZ5Y8OyZctKfRYDxWPD+fPny125quR4S0tL7fkrQy6XY9iwYfjtt98QFhaGoqKicseGR88BAKdOncKJEydK7afLuDR48GAAwJIlS0ptX7RoEQBwbKgHOGNBomrdujU2b96MN998Ex4eHto7bwuCgLi4OGzevBlyuVzboPeoHTt2lNv43b9/fzg6OmofHzp0CAUFBWX2CwgIwLvvvov169cjMDAQ48aNQ5cuXXD//n1s3boVly5dwsaNG7XNbRUZOnQo5s6di7Fjx8LPzw8XL17Epk2bSn2TDwCjRo3Cxo0bMXXqVJw+fRo9e/ZEbm4uDh48iPfffx8vv/wyzM3N4enpia1bt6JNmzaws7NDhw4d0KFDhwrP//rrr+Pjjz/Gxx9/DDs7uzLf1g0ZMgSLFi3CwIED8dZbbyElJQUrVqxAq1atyvQ4eHt74+DBg1i0aBGaNm0Kd3f3cpcCbty4MYKDgzFnzhwMHDgQL730EmJjY7Fy5Uo888wzFfbFEBHVlOnTpyMwMBChoaGYOHEiVqxYgeeeew4dO3bEhAkT0KJFCyQnJ+PEiRO4e/dumXsL/fLLL4iJiSnzuqNHj8aMGTOwfft29OrVC++99x7atm2LhIQEhIaGIjExsVILhQwdOhRhYWGwtbWFp6cnTpw4gYMHD5bqvSvJY8eOHdpxyNvbG+np6fj111+xevVqeHl5oWXLlmjQoAFWr14Na2trWFpaonv37k/shRg+fDiWLVuG2bNno2PHjqVmEEri27lzJ1555RUMGTIEcXFxWL16NTw9PUv1PuoyLnl5eWH06NFYu3YtMjMz0bt3b5w+fRo//vgjAgICyr33EhkYcRajIirt+vXrwqRJk4RWrVoJZmZmgrm5udC2bVth4sSJQnR0dKl9n7TcLB5ZArBkudmKfsLCwgRBKF5e76OPPhLc3d0FExMTwcbGRujbt6/wxx9/VCr2goICYdq0aUKTJk0Ec3NzoUePHsKJEyeE3r17C7179y61b15envB///d/2nM5OTkJw4YNE27cuKHd5/jx44K3t7dgampaaom/x5f1e1SPHj3KXeKvxA8//CC0bt1aUCgUQtu2bYUNGzaU+3oxMTFCr169BHNzcwGAdunZx5ebLbF8+XKhbdu2gomJieDo6ChMmjRJyMjIKLVP7969hfbt25eJ6fElEomIHlfy2XPmzJkyz6nVaqFly5ZCy5YthaKiIkEQBOHGjRvCqFGjBCcnJ8HExERwdnYWhg4dKuzYsUN7XMlysxX9HD16VBAEQbh7964wfvx4wdnZWTA2Nhbs7OyEoUOHCidPnqxU7BkZGcLYsWMFe3t7wcrKSvD39xdiYmIENze3Mst6379/X5g8ebLg7OwsmJqaCi4uLsLo0aOFtLQ07T579uwRPD09BWNj41JLz1b0WarRaARXV1cBgPDf//633Ofnz58vuLm5CQqFQujSpYuwd+/ecl9Pl3FJpVIJc+bM0Y5zrq6uQnBwcKllgAWh4uXeyxs7SX/IBIEdMkREREREVD3ssSAiIiIiompjYUFERERERNXGwoKIiIiIiKqNhQUREREREVUbCwsiIiIiIqo2FhZERERERFRt9e4GeRqNBgkJCbC2ttbpNvVERIZMEATk5OSgadOmkMvr73dOHCOIiErTZXyod4VFQkICXF1dxQ6DiEiS7ty5U+6d7usLjhFEROWrzPhQ7woLa2trAMW/HBsbG52OValUOHDgAAYMGAATE5PaCK9OGEIezEE6DCEPQ8gBqF4e2dnZcHV11X5G1lf1fYxgDtJhCHkYQg6AYeRRV+NDvSssSqa2bWxsqjRoWFhYwMbGRm//wwIMIw/mIB2GkIch5ADUTB71/fKf+j5GMAfpMIQ8DCEHwDDyqKvxof5eSEtERERERDWGhQUREREREVWbqIXFqlWr0KlTJ+2Us6+vL/74448nHrN9+3a0bdsWZmZm6NixI/bt21dH0RIRUV3h+EBEpH9ELSxcXFzw1VdfITIyEmfPnsXzzz+Pl19+GZcvXy53/+PHj+PNN9/EO++8g3PnziEgIAABAQG4dOlSHUdORES1ieMDEZH+EbWwePHFFzF48GC0bt0abdq0wZdffgkrKyucPHmy3P2XLl2KgQMHYvr06WjXrh3mzZuHrl27Yvny5XUcORER1SaOD0RE+kcyq0Kp1Wps374dubm58PX1LXefEydOYOrUqaW2+fv7Y/fu3RW+rlKphFKp1D7Ozs4GUNwdr1KpdIqxZH9dj5MaQ8iDOUiHIeRhEDmoNZi79wraqKuWh5Rzr63xgYiovjh6LQ1/JcgwSBBq9TyiFxYXL16Er68vCgoKYGVlhV27dsHT07PcfZOSkuDo6Fhqm6OjI5KSkip8/ZCQEMyZM6fM9gMHDsDCwqJKMYeHh1fpOKkxhDyYg3QYQh76nMO2m3L8L1mORgoj2JqGw1jH+ei8vLzaCawaant8APjl0+OYg3QYQh6GkAOg/3ncTs/DlG0XkF1gBJ8z8Xijm5tOx+uSt+iFhYeHB6Kjo5GVlYUdO3Zg9OjR+PvvvyscPHQVHBxc6luskpt8DBgwoEprlIeHh6N///56u44xYBh5MAfpMIQ89D2Hn07F438nYiAD8EpzDQb5655HyR/UUlLb4wPAL58qwhykwxDyMIQcAP3MQ6kGFl8yQnaBDG5WAixSLmPfvvJ71SqiyxdPohcWpqamaNWqFQDA29sbZ86cwdKlS7FmzZoy+zo5OSE5ObnUtuTkZDg5OVX4+gqFAgqFosx2ExOTKv8BUZ1jpcQQ8mAO0mEIeehjDkevpeK/+2IBANP6t4brg6tVykOKedf2+ADwy6fHMQfpMIQ8DCEHQH/zEAQBU7ZdQGJeMhpZmmJcm7xa/+JJ9MLicRqNptS09KN8fX1x6NAhTJkyRbstPDy8wmtuiYgM2c3UBwjaFAW1RsCrXZ3xbs/m+OOPq2KHVWtqY3zgl0/lYw7SYQh5GEIOgP7lsfrvG9h3KRnGchmWv+mFlMsnav2LJ1ELi+DgYAwaNAjNmjVDTk4ONm/ejIiICPz5558AgFGjRsHZ2RkhISEAgA8//BC9e/fGwoULMWTIEGzZsgVnz57F2rVrxUyDiKjOZeWpMP7Hs8guKELXZg0w/5WOkEEjdlg1huMDEVHVHfknFd/sjwEAzH6pPXzcGkLHK6CqRNTCIiUlBaNGjUJiYiJsbW3RqVMn/Pnnn+jfvz8AID4+HnL5vx2Ifn5+2Lx5Mz777DPMnDkTrVu3xu7du9GhQwexUiAiqnNFag0m/xyFm2m5aGprhjUjfWBmYgSVynAKC44PRERVE38/Dx/8fA4aAQj0dsGI7s1QVFRUJ+cWtbD44Ycfnvh8REREmW2BgYEIDAyspYiIiKTvv79fxdFraTA3McL3o33Q2LrspTz6juMDEZHu8gqL8G7YWWTlq+Dl2gDzAjpAJpPV2flFvUEeERHpZvOpeIQevwUAWDzcC+2b2oobEBERSYIgCPj0l4uIScqBvZUpVo/oCjMTozqNgYUFEZGeOHHjPmbtuQQAmNa/DQZ2aCJyREREJBXrjsbht/MJMJbLsPJtbzSxNa/zGFhYEBHpgfj7eZi0KRJFGgEvejXF5OdbiR0SERFJxLFraQh5uCrg50M90c3dTpQ4WFgQEUlcToEK4zeeQWaeCp1cbLFgWKc6vWaWiIik6056Hib/HAWNAAzzdsEoX93urF2TWFgQEUmYWiNgypZo/JP8AI42Cnw/yqfOr5klIiJpyi9U472wSO0XT/+t42btx7GwICKSsAV/xuJQTAoUxnKsHekDRxszsUMiIiIJEAQBM3ZewJXEbDSyNMXqEd6if/HEwoKISKJ2Rt3F6r9vAAC+GdYJXq4NxA2IiIgk44djcdgTnQAjuQwr3u6Kpg3qvln7cSwsiIgk6Fx8BmbsvAgACOrbEi93dhY5IiIikorj19MQ8kfxnbU/G9IOz7ZoJHJExVhYEBFJTGJWPt4Ni0RhkQb9PR0xrb+H2CEREZFE3M3Iw+Sfz0GtEfBqV2eM8WsudkhaLCyIiCSkQKXGuxsjkZqjRFsnaywZ3hlyOVeAIiKi4jHivbBIpOcWooOzDea/0lFSqwSysCAikghBEDB9xwVcvJcFO0tTfD/KB5YKY7HDIiIiCRAEATN3XsTlhGzYSaRZ+3EsLIiIJGJlxI1H7praFa52FmKHREREEhF6/BZ2nrsHI7kMy9/qApeG0hsjWFgQEUlA+JVkfHsgFgAw5+X2kmnEIyIi8Z28eR///b34ztozB7eDX0t7kSMqHwsLIiKRxSblYMqWcxAEYJSvG97uLt5dU4mISFruZeYjaFMU1BoBAZ2bYlyP5mKHVCEWFkREIsrILcT4jWeQW6iGb4tG+Hyop9ghERGRRBSo1Jj0UyTu5xbCs4kNQl7tJKlm7cexsCAiEolKrcH7m6JwJz0frnbmWPl2V5gY8WOZiIiKm7X/b9clXLibhYYWJlgz0hvmptJq1n4cRzAiIpH8d+8VnLh5H5amRlg36hk0tDQVOyQiIpKIjSdu45eou5DLgOVv6ceCHiwsiIhE8PPpePx44jYAYPHwzvBwshY5IiIikopTN+9j3t4rAIDgQe3Qo5U0m7UfJ2phERISgmeeeQbW1tZwcHBAQEAAYmNjn3hMaGgoZDJZqR8zM7M6ipiIqPrO3ErHrD2XAAAfD2iDAe2dRI6IiIikIjErH0Gbo1CkEfCSV1OM7+kudkiVJmph8ffffyMoKAgnT55EeHg4VCoVBgwYgNzc3CceZ2Njg8TERO3P7du36yhiIqLquZeZj4lhkVCpBQzp1ARBfVuJHRIREUlEgUqNiWGRSHtQiHZNbPD1a9Ju1n6cqIXF/v37MWbMGLRv3x5eXl4IDQ1FfHw8IiMjn3icTCaDk5OT9sfR0bGOIiYiqrr8QjXeCzurXd1jwTD9GjDqEme0iai+EQQBn+++hPN3s2BrboI1I6TfrP04SfVYZGVlAQDs7OyeuN+DBw/g5uYGV1dXvPzyy7h8+XJdhEdEVGWCIODTXy7g0r1s2FmaYu0ob1iYGosdlmRxRpuI6pufTsVje2RJs3YXNGsk/Wbtx0lmVNNoNJgyZQp69OiBDh06VLifh4cH1q9fj06dOiErKwvffvst/Pz8cPnyZbi4uJTZX6lUQqlUah9nZ2cDAFQqFVQqlU4xluyv63FSYwh5MAfpMIQ86iKHtUfj8Ov5BBjLZfhueCc4WpnU+Pmqk4fU3r/9+/eXehwaGgoHBwdERkaiV69eFR5XMqNNRKRPztxKx5xfi78o/3RgW/Rs3VjkiKpGMoVFUFAQLl26hGPHjj1xP19fX/j6+mof+/n5oV27dlizZg3mzZtXZv+QkBDMmTOnzPYDBw7AwqJqlWB4eHiVjpMaQ8iDOUiHIeRRWzlcyZBhbYwcgAwBbkW4f/Uk9l2tlVMBqFoeeXl5tRBJzdF1Rluj0aBr166YP38+2rdvXxchEhFVSXJ2Ad7fVNysPaRTE7zbq4XYIVWZJAqLyZMnY+/evThy5Ei5sw5PYmJigi5duuD69evlPh8cHIypU6dqH2dnZ8PV1RUDBgyAjY2NTudSqVQIDw9H//79YWJiotOxUmIIeTAH6TCEPGozh7i0XHy25hQEFGG4jwvmvdSu1voqqpNHyWyuFNXWjDbAWe3HMQfpMIQ8DCEHoHbzUBZp8F7YWaTmKOHhaIUvX2qHoqKiGj9PXc1oi1pYCIKADz74ALt27UJERATc3XVfTkutVuPixYsYPHhwuc8rFAooFIoy201MTKr8B0R1jpUSQ8iDOUiHIeRR0znkFKgwaXM0cgqK4OPWEPMCOsLUuPZb26qSh5Tfu9qa0QY4q10R5iAdhpCHIeQA1E4eW27IEZ0ih4WRgNebZuLvQwdq/ByPqu0ZbVELi6CgIGzevBl79uyBtbU1kpKSAAC2trYwNzcHAIwaNQrOzs4ICQkBAMydOxfPPvssWrVqhczMTCxYsAC3b9/G+PHjRcuDiOhxGo2Aj7ZG40ZqLprYmmHVCO86KSoMTW3OaAOc1X4cc5AOQ8jDEHIAai+PLWfu4sSJK5DJgOVve6Nn69q7CV5dzWiLWlisWrUKANCnT59S2zds2IAxY8YAAOLj4yGX/zsYZ2RkYMKECUhKSkLDhg3h7e2N48ePw9PTs67CJiJ6qsUH/8HBqylQGMuxZqQ3GluXnTmlitXFjDbAWe2KMAfpMIQ8DCEHoGbziLydgbm/FzfbTff3wPOeTWrkdZ+mtme0Rb8U6mkiIiJKPV68eDEWL15cSxEREVXfHxcTseyv4m/JQ17tiE4uDcQNSA9xRpuIDFVydgEm/VR8o9TBHZ0wqXdLsUOqMZJo3iYiMhQxSdmYtv08AOCd59zxalfdLt+hYpzRJiJDVFikwaSfIpGSo0QbRyssGOZlUDdKZWFBRFRDMvMK8e7GSOQVquHXshGCB7UVOyS9xRltIjJEc367jKj4TNiYGWPtSB9YKgzrT3F2EhIR1QC1RsAHP59DfHoeXBqaY/lbXWFsxI9YIiIqtuV0PDadiodMBix9owua21uKHVKN46hHRFQDFvwZi6PX0mBmIsfakT6wszQVOyQiIpKIqPgMzNpTfGftjwd4oG9bB5Ejqh0sLIiIqmnvhQSs/vsGAGDBMC94NtVtmVIiIjJcKTnFzdqFag0GtnfC+30Mp1n7cSwsiIiq4WpiNqZvvwAAeK93C7zo1VTkiIiISCoKizQI2hSF5GwlWjtY4dvXDatZ+3EsLIiIqigzrxDvhUUiX6VGz9b2+MSfzdpERPSveXuv4MytDFgrjLFmpDesDKxZ+3EsLIiIqkCtEfCfLdGIT8+Dq505lr3ZBUZyw/0WioiIdLPtzB2Enbxd3Kz9Zme0aGwldki1joUFEVEVLDwQiyP/pMLMRI41I3zQwILN2kREVCz6TiY+230JAPBRvzZ4vq2jyBHVDRYWREQ6+uNiIlZGFDdrf/1aJzZrExGRVmqOEhPDipu1B3g6YnLfVmKHVGdYWBAR6eBacg4+fnhn7fHPuePlzs4iR0RERFKhUhc3aydlF6BlY0ssfN0L8np0mSwLCyKiSsouUOG9sEjkPryz9gzeWZuIiB7x5e9XcfpWOqwUxlg7ygfWZiZih1SnWFgQEVWCRiNg6tbzuJmWC+cGxc3avLM2ERGV2BF5F6HHbwEAFg/vjJb1oFn7cRwViYgqYfnh6zh4NRmmxnKsGtEVjawUYodEREQSceFuJmbuuggAmNKvNfp71o9m7cexsCAieorDMSlYfPAfAMB/Azqgk0sDcQMiIiLJSHvwsFm7SIN+7Rzwn+dbix2SaFhYEBE9we37ufhwyzkIAvB292Z43cdV7JCIiEgiSpq1E7IK0KKxJRYN71yvmrUfx8KCiKgC+YVqTPwpCtkFRejSrAFmvegpdkhERCQh8/ddxam4h83aI31gU8+atR/HwoKIqByCIGDmrou4mpgNeytTrHrbGwpjI7HDIiIiidgZdRcb/ncLALDwdS+0cqh/zdqPY2FBRFSOjSduY9e5ezCSy7D8ra5wsjUTOyQiIpKIS/eyELyzuFn7P8+3gn97J5EjkgZRC4uQkBA888wzsLa2hoODAwICAhAbG/vU47Zv3462bdvCzMwMHTt2xL59++ogWiKqLyJvp2Pe3isAgOBBbfFsi0YiR0RERFJx/4ES74VFQlmkwQttHTClXxuxQ5IMUQuLv//+G0FBQTh58iTCw8OhUqkwYMAA5ObmVnjM8ePH8eabb+Kdd97BuXPnEBAQgICAAFy6dKkOIyciQ5WSU4D3N0WhSCNgSKcmeOc5d7FDIiIiiShSazB58zncy8yHuz2btR9nLObJ9+/fX+pxaGgoHBwcEBkZiV69epV7zNKlSzFw4EBMnz4dADBv3jyEh4dj+fLlWL16da3HTESGS/VwwEjOVqK1gxW+ea0TZDIOGEREVCzkjxicuHkflqZGWDPSG7bm9btZ+3GiFhaPy8rKAgDY2dlVuM+JEycwderUUtv8/f2xe/fucvdXKpVQKpXax9nZ2QAAlUoFlUqlU3wl++t6nNQYQh7MQToMIY+S2L/ZH4vTcemwVBhh2RteMJULepVXdd4LqeUZEhKCnTt3IiYmBubm5vDz88PXX38NDw+PJx63fft2fP7557h16xZat26Nr7/+GoMHD66jqInIkO2JTsAPx+IAFDdrt3G0Fjki6ZFMYaHRaDBlyhT06NEDHTp0qHC/pKQkODqWvpuho6MjkpKSyt0/JCQEc+bMKbP9wIEDsLCwqFKs4eHhVTpOagwhD+YgHfqex7n7MoT+cwcAMNytELFn/sbTO76kqSrvRV5eXi1EUnUll8o+88wzKCoqwsyZMzFgwABcuXIFlpaW5R5TcqlsSEgIhg4dis2bNyMgIABRUVFPHFeIiJ7mbi7w3Z7i3rvJfVthYIcmIkckTZIpLIKCgnDp0iUcO3asRl83ODi41AxHdnY2XF1dMWDAANjY2Oj0WiqVCuHh4ejfvz9MTPR36ssQ8mAO0mEIecQmZuKT1acAAOOfa45P/fWzEa8670XJbK5U8FJZIpKK9NxC/BBrBGWRBn08GuOj/vo5RtQFSRQWkydPxt69e3HkyBG4uLg8cV8nJyckJyeX2pacnAwnp/KX+VIoFFAoFGW2m5iYVPmPoOocKyWGkAdzkA59zSNXWYQp2y9DqZGhW/OGmDGoHYyN9Hsl7qq8F1J/72rjUlkioqcpUmvw0bYLSFfK0MzOHEuHd4ERm7UrJGphIQgCPvjgA+zatQsRERFwd3/66iu+vr44dOgQpkyZot0WHh4OX1/fWoyUiAyRIAiYsfMirqfmwsZEwJLXO+l9UWGIautSWYB9eI9jDtJhCHkYQg5f7Y/F8ZvpMJULWPZ6B1iY6Gc+ddWDJ2phERQUhM2bN2PPnj2wtrbWfvjb2trC3NwcADBq1Cg4OzsjJCQEAPDhhx+id+/eWLhwIYYMGYItW7bg7NmzWLt2rWh5EJF++vH4Lfx2PgHGchnGtilCY+uys5skvtq6VBZgH15FmIN0GEIe+ppDVJoMP14zAgC83UqDW+dP4NZ5kYOqptruwRO1sFi1ahUAoE+fPqW2b9iwAWPGjAEAxMfHQy7/9xtEPz8/bN68GZ999hlmzpyJ1q1bY/fu3WzMIyKdRMVn4Mt9VwEAn/i3gWPmZZEjovLU5qWyAPvwHsccpMMQ8tDnHK4m5uDT708B0GB8j2boqLmpl3mUqKsePNEvhXqaiIiIMtsCAwMRGBhYCxERUX1w/4ESQZuioFILGNKxCcb4NsMff7CwkJK6ulSWfXjlYw7SYQh56FsOGbmFCNoSjQKVBj1b2+PjAR74c/9NvcujPLXdgyeJ5m0iorqi1giYsjUaiVkFaNHYEl+91hG8B5708FJZIhJDkVqD/2w5hzvp+WhmZ4Flb7JZWxfsUiSiemXpoWs4ei0N5iZGWD3CG9Zm+v3tk6FatWoVsrKy0KdPHzRp0kT7s3XrVu0+8fHxSExM1D4uuVR27dq18PLywo4dO3ipLBHpZMGBWO0YsWakNxpYmIodkl6p0oxFXFwcjh49itu3byMvLw+NGzdGly5d4OvrCzMzs5qOkYioRkTEpmDZX9cAAPNf7cC7pkoYL5Ulorq290IC1vx9EwCwILAT2jXRrc+KdCwsNm3ahKVLl+Ls2bNwdHRE06ZNYW5ujvT0dNy4cQNmZmZ4++238emnn8LNza22YiYi0tm9zHxM2RoNQQDe7t4Mr3R5ciMwERHVH1cTszF9+wUAwHu9WmBop6YiR6SfKl1YdOnSBaamphgzZgx++eUXuLq6lnpeqVTixIkT2LJlC3x8fLBy5Up+a0REklBYpMH7m6KQmadCJxdbzHrRU+yQDBpntYlIn2TmFeK9sEjkq9To2doenwxsK3ZIeqvShcVXX30Ff3//Cp9XKBTo06cP+vTpgy+//BK3bt2qifiIiKpt/r6rOH8nE7bmJljxVlcojI3EDskgcVabiPSNWiPgP1uiEZ+eB5eG5vjuDTZrV0elC4snFRWPa9SoERo1alSlgIiIatLvFxIRevwWAGDR615wtavaTc/oyTirTUT6aOGBWBz5JxVmJnKsGemNhpZs1q6OKq0KFRoaWu72oqIiBAcHVyceIqIaczP1AT79pfia2Ul9WuKFdo4iR2S4vvrqK5w6dQrvv/9+maIC+HdWe/Xq1YiJiUGLFi1EiJKI6F/7LiZiZcQNAMDXr3VC+6a2Ikek/6pUWPznP/9BYGAgMjIytNtiY2PRvXt3/PzzzzUWHBFRVeUXqvH+pig8UBahm7sdpvVvI3ZIBk3XWW1vb+9ajIaI6Mlik3Lw8fbzAIAJPd3xcmdnkSMyDFUqLM6dO4e7d++iY8eOCA8Px4oVK9C1a1e0bdsW58+fr+kYiYh0NvvXS4hJyoG9lSmWv9kFxka8bU9d4aw2EUlZVp4K74WdRV6hGn4tG+FTNmvXmCqNtC1btsT//vc/vPrqqxg4cCA++ugjrFu3Dps2bYKtLaeRiEhc28/ewbazdyGXAd+90QUONlyJqC5xVpuIpEqtEfDh1nO4dT8Pzg3MsfytrvziqQZV+Tf5+++/Y8uWLfD19UWDBg3www8/ICEhoSZjIyLSWWxSDj7fcwkA8FG/NvBrZS9yRPUPZ7WJSKoWh/+DiNhUKIyLm7Xt2Kxdo6pUWLz33nsIDAzEp59+iqNHj+LChQswNTVFx44dsW3btpqOkYioUnKVRZi0KRIFKg16tWmMoL6txA6pXuKsNhFJ0f5LiVh++DoA4KvXOqKDMz+PalqVCov//e9/OHXqFKZNmwaZTAYnJyfs27cPc+fOxbhx42o6RiKipxIEATN3XcTN1Fw42ZhhyfDOkHMtctFwVpuIpORacg6mbSueMR3Xwx2vdHEROSLDVKXCIjIyEl5eXmW2BwUFITIystpBERHp6ufTd7AnOgFGchmWv9WF09si4qw2EUlJVr4K74ZFIrdQjWdb2CF4MJu1a0ulb5D3KIVCUeFzHh4eVQ6GiKgqLt3Lwhe/XQYAfOLvAZ/mdiJHVL+VzGqXfAFVMqu9YsUKjBs3Dq+//rrIERJRfaHRCPhoazTi0nLR1NYMK97qChM2a9eaSv9mBw4ciJMnTz51v5ycHHz99ddYsWJFtQIjIqqMnAIVJm+OQmGRBi+0dcCEnrzxmtg4q01EUrHk0DX8FZPysFnbB42sKv5ynKqv0jMWgYGBeO2112Bra4sXX3wRPj4+aNq0KczMzJCRkYErV67g2LFj2LdvH4YMGYIFCxbUZtxERBAEATN2XtQuG7jwdS/2VUgAZ7WJSAr+vJyE7w5dAwDMf6UjOrqwWbu2VXrG4p133sHNmzcxc+ZMXLlyBe+++y569uyJZ555Bv7+/vj+++/RrFkznDlzBlu3bkWzZs2e+ppHjhzBiy++iKZNm0Imk2H37t1P3D8iIgIymazMT1JSUmXTICID8tPJ2/j9QiKM5TIse6sLGliwr0IsnNUmIim5nvJvs/YYv+Z4zZvN2nVBpx4LhUKBESNGYMSIEQCArKws5Ofno1GjRjAxMdH55Lm5ufDy8sK4cePw6quvVvq42NhY2NjYaB87ODjofG4i0m8X72Zh3t6rAIAZg9qia7OGIkdUv3FWm4ikIruguFn7gbII3d3t8H9D2okdUr1RpebtEra2ttVak3zQoEEYNGiQzsc5ODigQYMGVT4vEem37AIVgjZHoVCtQX9PR7zznLvYIdV777zzDkaMGIHt27dj69atWLt2LbKysgAAMpkMnp6e8Pf3x5kzZ9CuHQd5IqodGo2AqVujcTM1F01szbDibTZr1yWdCovvvvuu3O22trZo06YNfH19aySop+ncuTOUSiU6dOiAL774Aj169KhwX6VSCaVSqX2cnZ0NAFCpVFCpVDqdt2R/XY+TGkPIgzlIR13nIQgCPtl+AfHpeXBuYIaQAE8UFRVV6zX5XtRM7jU9q01EpKvv/rqGg1dTYGosx+oR3rBns3ad0qmwWLx4cbnbMzMzkZWVBT8/P/z666+ws6udpR6bNGmC1atXw8fHB0qlEuvWrUOfPn1w6tQpdO3atdxjQkJCMGfOnDLbDxw4AAsLiyrFER4eXqXjpMYQ8mAO0lFXeRxNkmF/nBGMZAKGuzzA/w7X3Hnr83uRl5dX43FUd1abiEgX4VeSseRgcbP2lwEd4OXaQNyA6iGdCou4uLgKn7t58yZGjBiBzz77DCtXrqx2YOXx8PAotaKIn58fbty4gcWLFyMsLKzcY4KDgzF16lTt4+zsbLi6umLAgAGl+jQqQ6VSITw8HP3799frb98MIQ/mIB11mcflhGx8vPYUAAGfDmyLsX5uNfK6fC/+nc2tjpqe1T5y5AgWLFiAyMhIJCYmYteuXQgICKhw/4iICPTt27fM9sTERDg5Oel0biLSLzdSH2Dq1mgAwGhfNwT6uIobUD1VrR6LR7Vo0QJfffUVxo0bV1MvWSndunXDsWPHKnxeoVCUu/ShiYlJlf+AqM6xUmIIeTAH6ajtPLILVPhw2wWo1AL6tXPEhF4tIZPV7NKy9fm9qIm8a3pWmwt8EFFl5BSo8O7Gs8hRFqFbczt8NtRT7JDqrRorLACgWbNmdb70a3R0NJo0aVKn5ySiuiUIAoJ/uYjbD+9X8W1gpxovKqj6anpWmwt8ENHTaDQCpm07jxupuXCyMcPyt7uwWVtENVpYXLx4EW5ulb804cGDB7h+/br2cVxcHKKjo2FnZ4dmzZohODgY9+7dw8aNGwEAS5Ysgbu7O9q3b4+CggKsW7cOf/31Fw4cOFCTaRCRxPx0Kh6/Xyy+X8Vy3q9CL9XlrLYuC3wQkX5bcfg6DlxJhqmRHKtHesPB2kzskOo1nQqLiq7BzcrKQmRkJKZNm4bRo0dX+vXOnj1b6nrYkl6I0aNHIzQ0FImJiYiPj9c+X1hYiGnTpuHevXuwsLBAp06dcPDgwXKvqSUiw3DpXhbm/XYFAPDpwLbowvtV6K3antWuygIfXDmwNOYgHYaQR23ncDg2FYsO/gMA+OLFdmjvZFkr56rv74Uux+hUWDRo0KDCyw9kMhnGjx+PGTNmVPr1+vTpA0EQKnw+NDS01ONPPvkEn3zySaVfn4j0W06BCpMf3q/ihbYOGN+T96vQZ7rOauuqKgt8cOXA8jEH6TCEPGojh5R8YNFFIwiCDD0cNbBMPo99+87X+HkeVV/fC11WDdSpsDh8+HC5221sbNC6dWuYmZkhJSUFTZs21eVliYjKEAQBM3ddwq37eWhqa4ZvA73YVyFxNT2rXROetsAHVw4sjTlIhyHkUVs5PFAWIXDNKeSrc+HdrAHWjvWBqXHt9VXU9/dCl1UDdSosevfu/cTnz58/j65du0KtVuvyskREZfx8+g5+O58AI7kMy97qgoaW7KuQupqe1a4JT1vggysHlo85SIch5FGTOQiCgOAtF3A9NReONgqsGukNS/O6uQlefX0vdNm/Rpu3iYhqwtXEbMz57TIAYLq/B7zdauemm1SzanpWmwt8ENHjVkbcwP7LSTAxkmHVCDZrSw0LCyKSlFxlEYI2R0FZpEEfj8Z4t2cLsUOiSqrpWW0u8EFEjzocm4JvD8QCAOa81AFduZiH5LCwICLJEAQBn+2+hJsP1yNf9HpnyOXsq6ivuMAHEZW4lZaLD38+B0EA3uzWDG91byZ2SFQOnQqLCxcuPPH52NjYagVDRPXb9rN3sevcPRjJZfjuzS6wY18FEVG9l6sswnthkcguKEKXZg3wxUu8s7ZU6VRYdO7cGTKZrNxvkEq2c9UWIqqKf5JzMOvXSwCAqf3boJs7+yqIiOo7QRDwyY4LiE3OQWNrBVaP8IbC2EjssKgCOhUWcXFxtRUHEdVjeYVFCNoUhQKVBj1b22NS75Zih0RVwFltIqppq/++id8vJhY3a7/dFY42bNaWMp0Ki9q8sRER1V+z91zGtZQHcLBWYPFw9lXoK85qE1FN+vufVHzzZwwAYPaL7eHTnDPZUqdTYfHNN9/ggw8+gLm5OQDgf//7H3x8fLRrgOfk5ODTTz/FypUraz5SIjJIv0TexfbIu5DLgKVvdIG9Vd2sR041j7PaRFRTbt/PxX8eNmsP93HF22zW1gs6FRbBwcEYM2aMtrAYNGgQoqOj0aJF8XKQeXl5WLNmDQsLIqqU6yk5+Gx3cV/FlH5t4NuykcgRUXVwVpuIakJeYXGzdla+Cp1dG2BuQHvOduoJne5//vj09pOWASQiepL8QjWCNp1DvkqNHq0aIahvK7FDohp09OhRjBgxAr6+vrh37x4AICwsDMeOHRM5MiKSspJm7ZikHNhbKbBqRFc2a+sRnQoLIqKa8sWvlxGbXDxwLBneBUbsqzAYv/zyC/z9/WFubo5z585BqVQCALKysjB//nyRoyMiKfv+6E3svZAIY7kMq0Z0RRNbc7FDIh2wsCCiOrcz6i62nr0DmQz47o3OaGzNvgpD8t///herV6/G999/DxMTE+32Hj16ICoqSsTIiEjKjl1Lw1d/lDRre+IZNmvrHZ3vvL1u3TpYWVkBAIqKihAaGgp7e3sAxc3bRERPcj0lB/+3q7iv4sMXWsOvlb3IEVFNi42NRa9evcpst7W1RWZmZt0HRESSdyc9D5N/joJGAF73ccGIZ9mzpY90KiyaNWuG77//XvvYyckJYWFhZfYhIirPo30Vfi0b4YPnW4sdEtUCJycnXL9+Hc2bNy+1/dixY9rFPoiISuQXqvFuWCQy81TwcrHF3Jc7sFlbT+lUWNy6dauWwiCi+mD2r5f+7at4ozP7KgzUhAkT8OGHH2L9+vWQyWRISEjAiRMnMG3aNMyaNUvs8IhIQgRBwKe/XMDVxGzYW5li9UhvmJmwWVtf6VRYFBQU4ODBgxg6dCiA4uVnS5ryAMDY2Bhz586FmRnvikhEpf0SeRfbzhbfr+K7NzrDwZqfE4ZqxowZ0Gg0eOGFF5CXl4devXpBoVBg+vTpGD9+vNjhEZGE/HAsDr+eT4CxXIYVb7FZW9/p1LwdGhqKNWvWaB8vX74cx48fx7lz53Du3DmEhYXpdA+LI0eO4MUXX0TTpk0hk8mwe/fupx4TERGBrl27QqFQoFWrVggNDdUlBSISwbXkf+9X8eELbdhXYeBkMhn+7//+D+np6bh06RJOnjyJ1NRU2Nrawt3dXezwiEgijl9Pw/x9VwEAnw1ph+4teC8jfadTYbFp0ya8++67pbZt3rwZhw8fxuHDh7FgwQJs37690q+Xm5sLLy8vrFixolL7x8XFYciQIejbty+io6MxZcoUjB8/Hn/++acuaRBRHcorLML7m6KQr1LjuVb2mPw871dhqJRKJYKDg+Hj44MePXpg37598PT0xOXLl+Hh4YGlS5fio48+EjtMIpKAO+l5CNpc3Kz9WlcXjPZrLnZIVAN0uhTq+vXr6Nixo/axmZkZ5PJ/a5Nu3bohKCio0q83aNAgDBo0qNL7r169Gu7u7li4cCEAoF27djh27BgWL14Mf3//Sr8OEdUNQRDw2e5LuJbyAI2tFVg8nH0VhmzWrFlYs2YN+vXrh+PHjyMwMBBjx47FyZMnsXDhQgQGBsLIiNdOE9V3+YVqvBcWiYw8FTq52OLLV9isbSh0KiwyMzNL9VSkpqaWel6j0ZR6vqadOHEC/fr1K7XN398fU6ZMqbVzElHVbT97Fzuj7kEuA5a92YX3qzBw27dvx8aNG/HSSy/h0qVL6NSpE4qKinD+/Hn+0UBEAIq/cJq56yKuJGajkaUpVo9gs7Yh0amwcHFxwaVLl+Dh4VHu8xcuXICLi0uNBFaepKQkODo6ltrm6OiI7Oxs5Ofnw9y8bMOPUqksVexkZ2cDAFQqFVQqlU7nL9lf1+OkxhDyYA7SUVEeMUk5+HxPcV/FRy+0grerjWRzNfT3Qpdjq+Pu3bvw9vYGAHTo0AEKhQIfffQRiwoi0lr/v1vYde4ejOQyLH+rK5o2YLO2IdGpsBg8eDBmzZqFIUOGlFn5KT8/H3PmzMGQIUNqNMDqCgkJwZw5c8psP3DgACwsLKr0muHh4dUNSxIMIQ/mIB2P5lGgBhZeMIKySIZ2DTRweRCDfftiRIyucgzxvaisvLy8ap9XrVbD1NRU+9jY2Fh7Q1UiouM3Sjdr+7Zks7ah0amwmDlzJrZt2wYPDw9MnjwZbdq0AVB8l9Xly5ejqKgIM2fOrJVAgeKbLiUnJ5falpycDBsbm3JnK4DiJXGnTp2qfZydnQ1XV1cMGDAANjY2Op1fpVIhPDwc/fv3h4mJie4JSIQh5MEcpOPxPARBwJRtF5BSkAwnGwV+nOSLhhamT38hERnqe6GLktnc6hAEAWPGjIFCUXzJW0FBASZOnAhLS8tS++3cubPa5yIi/XIvMx+TN5+DWiPg1S7OGMNmbYOkU2Hh6OiI48ePY9KkSZgxYwYEQQBQvLRg//79sXLlyjKXKtUkX19f7Nu3r9S28PBw+Pr6VniMQqHQDnKPMjExqfIfENU5VkoMIQ/mIB0leYT+Lw77LiXDWC7DyhHecLC1fPrBEmFo74Wux1TX6NGjSz0eMWJEtV7vyJEjWLBgASIjI5GYmIhdu3YhICDgicdERERg6tSpuHz5MlxdXfHZZ59hzJgx1YqDiKqnQKXGxLBIpOcWooOzDea/2pGXSBoonQoLAHB3d8f+/fuRnp6O69evAwBatWoFOzs7nU/+4MED7WsAxcvJRkdHw87ODs2aNUNwcDDu3buHjRs3AgAmTpyI5cuX45NPPsG4cePw119/Ydu2bfj99991PjcR1byo+Ax8+XCae+bgdujarKHIEVFd2rBhQ42+XsmS5OPGjcOrr7761P1LliSfOHEiNm3ahEOHDmH8+PFo0qQJVw4kEokgALN+vYKL97Jgx2Ztg6dzYVHCzs4O3bp1q9bJz549i759+2ofl1yyNHr0aISGhiIxMRHx8fHa593d3fH777/jo48+wtKlS+Hi4oJ169ZxwCCSgPTcQkzeFAWVWsDgjk4Y26O52CGRnuOS5ET672iSDLtuJT5s1u4Cl4ZV628l/VDlwqIm9OnTR3s5VXnKu6t2nz59cO7cuVqMioh0pRGAaTsuIiGrAO72lvj6tU6c5qY6V5UlyblyYGnMQToMIY8T11Ox61bx/c4+9W+DZ5rZ6mU+hvBe1NWqgaIWFkRkGP68K8exu/dhZiLHqhFdYW2m/30KpH+qsiQ5Vw4sH3OQDn3NI0MJfHvBCBrI4G2vgUPGZezbd1nssKpFX9+LR9X2qoEsLIioWo5cS8Ofd4tnJ0Je7Yi2TrqttkYkJq4cWBpzkA59zkOpUuOtH87gQVE2nC0ErJ3QBzYWZk8/UKL0+b0oUVerBrKwIKIqu5uRh2nbL0KADG91c8ErXWrvBplET1OVJcm5cmD5mIN06FsegiBg5u4ruHAvGw3MTfCORz5sLMz0KoeK6Nt7UZ7aXjVQrmtARERA8fKBk36KQma+Cs0sBcwc1FbskKie8/X1xaFDh0pte9qS5ERUs346eRvbI+9CLgOWDO+ERvo7UUFVwMKCiHQmCAJm7bmEi/ey0NDCBGM91FAY8+OEataDBw8QHR2N6OhoAP8uSV6yWmBwcDBGjRql3X/ixIm4efMmPvnkE8TExGDlypXYtm0bPvroIzHCJ6p3TselY85vVwAAnw5six68s3a9w78EiEhnW87cwbazD7+Rer0T7MpeSUJUbWfPnkWXLl3QpUsXAMVLknfp0gWzZs0CgAqXJA8PD4eXlxcWLlzIJcmJ6khiVj7e3xSJIo2AF72a4t1eLcQOiUTAHgsi0sm5+AzM3lO8ssfH/h7wa9kI+2JFDooMEpckJ9IPBSo1Jv4UhbQHhWjrZI2vX+OdtesrzlgQUaWl5BRg0k9RKFRr4N/eEZN6txQ7JCIiEpEgCJi95zLO38mErbkJ1o70gYUpv7eur1hYEFGlFBZpELQpCknZBWjZ2BLfBnrxGykionpu06l4bD17B3IZsOzNLmjWiHfWrs9YWBBRpXz5+xWcuZUBK4Ux1o7y4U3wiIjqubO30jHnt+JLYz8Z2Ba92jQWOSISGwsLInqqbWfv4McTtwEAi4d3RsvGViJHREREYkrKKsDEn6KgUgsY0rEJ3mOzNoGFBRE9RVR8Bj7bdQkA8OELrdHf01HkiIiISEzKIjUmbYpE2gMlPByt8c2wTrw0lgCwsCCiJ0jOLsDEsEgUqjUY4OmID19oLXZIREQksi9+vYxz8ZmwMTPGmpHesFSwWZuKsbAgonIVqNR4NywSKTlKtHG0wqLhnSGX8xspIqL6bPOpePx8+g5kMuC7N7ugub2l2CGRhLCwIKIyBEFA8M6L2uUDvx/lAyt+I0VEVK9F3s7A7F+LL439eIAH+ng4iBwRSQ0LCyIqY9XfN7Dr3D0YyWVY+XZXuDXiN1JERPVZcnYBJv0UCZVawKAOTni/D+9jRGWxsCCiUg5cTsKCP4tvpf3Fi57o0cpe5IiIiEhMhUUaTPqp+NLY1g5WWMD7GFEFWFgQkdaVhGxM2RoNQQBGPNsMI32bix0SERGJbM5vlxEVnwlrs+L7GPHSWKoICwsiAlA8zf3Oj2eQV6iGX8tGmP1ie7FDIiIikW05HY9Np+KLm7Xf6AJ3NmvTE0iisFixYgWaN28OMzMzdO/eHadPn65w39DQUMhkslI/ZmZmdRgtkeHJKyzC+B/PIjGrAC0bW2LV294wMZLExwMREYkkKj4Ds/YU31l7ar826NuWzdr0ZKL/5bB161ZMnToVs2fPRlRUFLy8vODv74+UlJQKj7GxsUFiYqL25/bt23UYMZFh0WgEfLQ1GhfvZcHO0hTrxzwDWwsTscMiIiIRpeQUN2sXqjXwb++IoL6txA6J9IDohcWiRYswYcIEjB07Fp6enli9ejUsLCywfv36Co+RyWRwcnLS/jg68k7ARFX15b6r+PNyMkyN5Fg70psrQBER1XOFRRoEbYpCcrYSrRyssPB13seIKkfU7pvCwkJERkYiODhYu00ul6Nfv344ceJEhcc9ePAAbm5u0Gg06Nq1K+bPn4/27cu/HlypVEKpVGofZ2dnAwBUKhVUKpVO8Zbsr+txUmMIeTCHmhF64jZ+OBYHAPjq1fbwcraul/9fGEIOQPXy0PfciajmzNt7BWduZcBaYYy1I73ZrE2VJup/KWlpaVCr1WVmHBwdHRETE1PuMR4eHli/fj06deqErKwsfPvtt/Dz88Ply5fh4uJSZv+QkBDMmTOnzPYDBw7AwsKiSnGHh4dX6TipMYQ8mEPVnb8vw4Z/5ABkeKmZGkZ3z2Hf3XNVfj2+F9JRlTzy8vJqIRIi0jfbztxB2MniS8wXD++MFo2tRI6I9InelaC+vr7w9fXVPvbz80O7du2wZs0azJs3r8z+wcHBmDp1qvZxdnY2XF1dMWDAANjY2Oh0bpVKhfDwcPTv3x8mJvp7Dboh5MEcqufs7QxsCo2EAA3e6uaCL4a2q/Ka5HwvpKM6eZTM5hJR/RV9JxOf7S6+s/ZH/dqgnycvNSfdiFpY2Nvbw8jICMnJyaW2Jycnw8nJqVKvYWJigi5duuD69evlPq9QKKBQKMo9rqp/QFTnWCkxhDyYg+5ik3Lw3k/noCzSoF87B8x9uSOMa2AFKL4X0lGVPAwhbyKqutQcJSaGFTdr9/d0xAfPs1mbdCdq87apqSm8vb1x6NAh7TaNRoNDhw6VmpV4ErVajYsXL6JJkya1FSaRwbibkYdR608hu6AI3m4NsezNrjVSVBARkf5SqTUI2hyFpOwCtGhsiUWve7FZm6pE9L8opk6diu+//x4//vgjrl69ikmTJiE3Nxdjx44FAIwaNapUc/fcuXNx4MAB3Lx5E1FRURgxYgRu376N8ePHi5UCkV64/0CJUetPIzlbidYOVvhhtA/MTY3EDovoiXifI6La9+XvV3E6Lh1WCmOsHekDazPOYFLViN5jMXz4cKSmpmLWrFlISkpC586dsX//fm1Dd3x8POTyf+ufjIwMTJgwAUlJSWjYsCG8vb1x/PhxeHp6ipUCkeRlF6gwav1p3EzNRVNbM2x8pxsaWJiKHRbRE5Xc52j16tXo3r07lixZAn9/f8TGxsLBofwbddnY2CA2Nlb7uKq9Q0T1xY7Iuwg9fgtAcbN2Kwc2a1PViV5YAMDkyZMxefLkcp+LiIgo9Xjx4sVYvHhxHURFZBjyC9V4J/QMLidko5GlKcLGd0cTW3OxwyJ6qkfvcwQAq1evxu+//47169djxowZ5R5Tcp8jInq6i3ezMHPXRQDAhy+0Rn82a1M1SaKwIKLaoSxS472fIovXIzczxsZ3uqEllw4kPVAX9zkCeK+jxzEH6ajtPO7nFuLdsLMoLNLgeY/GeL9X8xo/F98L6air+xyxsCAyUIVFGrz/UxSO/JMKcxMjhI59Bu2b2oodFlGl1MV9jgDe66gizEE6aiMPtQZYeVWOxGw5HMwEDLBJxP79iTV+nhJ8L6Sjtu9zxMKCyACp1BpM3hyFQzEpUBjL8cNoH3i72YkdFlGt0vU+RwDvdfQ45iAdtZnHl/ticD07HpamRvhxQvda66vgeyEddXWfIxYWRAZGpdbgwy3ncOBKMkyN5fh+lA/8WtmLHRaRTuriPkcA73VUEeYgHTWdx65zdxF6Ih4AsPD1zmjn3LDGXrsifC+ko7bvcyT6crNEVHMKi4pnKvZdTIKpkRxrRnqjV5vGYodFpDPe54io5l26l4UZvxQ3a0/u2woDO3ChA6pZnLEgMhAFKjXe3xSFv2JSYGosx+oRXdHXo/wlOYn0wdSpUzF69Gj4+PigW7duWLJkSZn7HDk7OyMkJARA8X2Onn32WbRq1QqZmZlYsGAB73NE9FB6biHeC4uEskiDvh6N8VH/NmKHRAaIhQWRAcgrLMJ7YZE4ei0NZiZyrB3pw5kK0nu8zxFRzSh62Hd3LzMfzRtZYMkbXWDEO2tTLWBhQaTnMvMKMS70DKLiM2FhaoQfRj8D35aNxA6LqEbwPkdE1ffVHzE4fuM+LEyNsHaUD2zN9btPgKSLhQWRHkvOLsCoH04jNjkHNmbG2DD2Ga7+REREWnui72HdsTgAwLeBXmjjaC1yRGTIWFgQ6akbqQ8wZsNp3EnPh4O1AmHvdIeHEwcMIiIqdjkhC5/+cgEA8H6flhjckQsZUO1iYUGkh87cSseEjWeRmaeCWyML/PROd7jaVe1mXkREZHgyHjZrF6g06N2mMaYN8BA7JKoHWFgQ6Zm9FxIwddt5FBZp0Nm1AdaN9oG9Vdl1+ImIqH4qUmvwwc/ncDcjH83sLPAdm7WpjrCwINITGo2ApYeuYemhawAA//aOWDK8C8xNjUSOjIiIpGTBn7E4dj0N5iZGWDvKG7YWbNamusHCgkgP5CqLMG3beey/nAQAGNfDHf83pB2/gSIiolJ+PZ+ANUduAgAWBHZCWycbkSOi+oSFBZHE3UrLxcSfIhGTlAMTIxm+DOiI159xFTssIiKSmKuJ2fhkx3kAwMTeLTG0U1ORI6L6hoUFkYTtv5SI6dsvIEdZBHsrBdaM7MrlZImIqIzMvEK8G3YWBSoNera2x3R/NmtT3WNhQSRByiI1vtkfix8erj3+TPOGWPZmVzjZmokcGRERSY1aI+CDn8/hTno+XO3M2axNomFhQSQx11Ny8J+fo3ElMRsA8G6vFpju7wETI7nIkRERkRQt+DMWR689bNYe6YOGlqZih0T1lCT+UlmxYgWaN28OMzMzdO/eHadPn37i/tu3b0fbtm1hZmaGjh07Yt++fXUUKVHt0WgEbDxxC0O+O4YridloaGGCtSO9MXNwOxYVRERUrt8vJGL13zcAAF8P64R2TdisTeIR/a+VrVu3YurUqZg9ezaioqLg5eUFf39/pKSklLv/8ePH8eabb+Kdd97BuXPnEBAQgICAAFy6dKmOIyeqObfScvHm9ycxa89lKIuKr4/9c0ovDGjvJHZoREQkUTFJ2fh4e3Gz9ru9WuAlLzZrk7hELywWLVqECRMmYOzYsfD09MTq1athYWGB9evXl7v/0qVLMXDgQEyfPh3t2rXDvHnz0LVrVyxfvryOIyeqPrUGWHfsFgYuPYJTcekwNzHC7Bc98ePYbnCwYT8FERGVLytPhffCIpGvUuO5Vvb4hM3aJAGi9lgUFhYiMjISwcHB2m1yuRz9+vXDiRMnyj3mxIkTmDp1aqlt/v7+2L17d7n7K5VKKJVK7ePs7OLr1lUqFVQqlU7x/hJ5BxdTZCiIugOFiQmM5DIYy2UwNpLBSC6DqZEcxnIZTIzkD39kMDGWw9RIDlNjORQPf4zlMshk4jVVleSta/5SYgg5HP0nBd9cMEJS/j8AAL8Wdpj3siea2VlArS6CWi1ygJVkCO+FIeQAVC8Pfc+dqD5RawT8Z8s53L6fB5eG5lj2ZhcY85JZkgBRC4u0tDSo1Wo4OjqW2u7o6IiYmJhyj0lKSip3/6SkpHL3DwkJwZw5c8psP3DgACwsLHSKd85pI+SrjbDpxlWdjnucDAJM5ND+mMoBU6OH/5QLUBih+EcOKIwBMyMBZkaAmRFgbgSYGwswNwIsjAFz4+LjqlKnhIeHVysPKdDHHFLzgb135Ii+Lwcgg6WxgJfcNOjeOAWXTqZAXy/q08f34nGGkANQtTzy8vJqIRIiqg2LwmPx9z+pMDORY81IbzZrk2QY/KpQwcHBpWY4srOz4erqigEDBsDGRrcGp31Z5xCfkIwGDRtBAFCkEVCkEaDWCFCpBRSpNVCpBajUGhRpiv9ZWKRB4cPtJQTIUKgBCjXlnUX3CsHUWI4G5iZoYG6ChpYmsLMwhZ2lKRpZmsLOyhT2lqZobK2AvZUpHKwVMIIG4eHh6N+/P0xMTHQ+nxSoVCq9yyHtgRLLD9/E1gt3UaQRIJcBPRw1+GZkL9jb6FbkSok+vhePM4QcgOrlUTKbS0TS9sfFRKw4/LBZ+7VOaN/UVuSIiP4lamFhb28PIyMjJCcnl9qenJwMJ6fym1adnJx02l+hUEChUJTZbmJiovPAu/zNLti3bx8GD35G52M1GgGFag2UKg2URWoUqDQoKFKjQKVGfqEaeSo1CgrVyC1UI7+wCLmFauQqi/BAWYRcZRFyCkp+VMgpKEJWvgpZ+SoUaQQUFmmQkqNESo7y6YEAsDEzhoXMCNtTL6BpA3M42Zqjqa0ZmjYwR9MG5nBuYA5zUyOd8hNLVd7HupaYlY/vj8Th59PxyFcVX9/Uu01jTOvXCnHnjsLexkLyOVSGPrwXT2MIOQBVy8MQ8iYydP8k52Daw2bt8c+54+XOziJHRFSaqIWFqakpvL29cejQIQQEBAAANBoNDh06hMmTJ5d7jK+vLw4dOoQpU6Zot4WHh8PX17cOIq46uVwGM7kRzEyMANTMAC4IAvIK1cjIK0RmngoZeYVIz/33J+1BIdIeKHH/gRKpD5RIyVZCWaRBdkERsiFD0vX7Fb62vZUpnBtawKWhOZrZWcC1oQWa2VnArZEFmjYw5413KuFqYjZC/3cLO8/d1c5YdXZtgE8HtoVvy0ZQqVSIOydykEREpBey8lV4d+NZ5BWq4deyEWYMait2SERliH4p1NSpUzF69Gj4+PigW7duWLJkCXJzczF27FgAwKhRo+Ds7IyQkBAAwIcffojevXtj4cKFGDJkCLZs2YKzZ89i7dq1YqYhCplMBkuFMSwVxnBp+PT9BUFAjrII9+4/wG8Hj8KtXSekPlAhIasASVkFSMjMx72MfOQoix4WJYU4fyezzOuYGMng2rC4yGhubwn3R36a2ppDXo+LjgKVGuFXkhF28jZOx6Vrt3d3t8Pk51vhuVb2ojbuExGR/lFrBEzZcg637ufBuYE5lr/Vlc3aJEmiFxbDhw9HamoqZs2ahaSkJHTu3Bn79+/XNmjHx8dDLv/3fx4/Pz9s3rwZn332GWbOnInWrVtj9+7d6NChg1gp6A2ZTAYbMxOYO1jBo4GAwV2cy738IStfhbsZebiTnv/wn3m4nZ6H+PQ83E3PR6Fag5tpubiZlgvEppY61tRYDvdGlmjR+OGPvRVaOlihRWNL2JgZ5qUWao2AqPgM7Dp3D3vPJyC7oAgAYCSXYWB7J4x7rjm83exEjpKIiPTVkoP/4HBsKhTGxc3admzWJokSvbAAgMmTJ1d46VNERESZbYGBgQgMDKzlqOovW3MT2JrbltsQptYISMouwO20XMTdz8WttFzEpeUhLu0B4tPzUFikQWxyDmKTc8oca2+lQIvGlmj5sOBwty8uPlztLPTuztK5yiKciruP8CvJCL+SgrQH//a3NLE1wzBvF7zd3Q1OtrwXBVF1rFixAgsWLEBSUhK8vLywbNkydOvWrcL9t2/fjs8//xy3bt1C69at8fXXX2Pw4MF1GDFRzTpwJRnL/roOAPjqtY7o4MxmbZIuSRQWpD+M5DI4P2zw9mtlX+q5IrUG9zLzcTM1FzdSHxTPaqQ+wM3UXKTkKJH2oPjn0UuESl7TtaE5mttbonkjS7g1Kr7MqpmdJVwamj/sSxFXem4hou9k4Fx8Jk7evI9z8Zko0vy70pe1mTH6ezpiWFcXPNuiUb2+HIyopmzduhVTp07F6tWr0b17dyxZsgT+/v6IjY2Fg4NDmf2PHz+ON998EyEhIRg6dCg2b96MgIAAREVFcVab9NK9XGDFL8WLkI/r4Y5XuriIHBHRk7GwoBpjbCSHWyNLuDWyRN+2pQf9nAIV4tJycTP1YbHx8N/j0nKRr1Lj1v083LqfByC1zOs6WCvg3LC4mGnawBxONmawtzTGzWzg9v08ODW0hKWpUbV7F1RqDZKyCnAnIw93M/JxI/UBriU/wD/JObibkV9m/2Z2FujVxh7+7Z3Q3b0RTI31a9aFSOoWLVqECRMmaHvuVq9ejd9//x3r16/HjBkzyuy/dOlSDBw4ENOnTwcAzJs3D+Hh4Vi+fDlWr15dp7ETVYeySI0Vf93AiotGUAtqPNvCDjMHs1mbpI+FBdUJazMTdHJpgE4uDUptFwQBydlK3Ex7gNv383Dr4eVV8en5iL+fi9xCtXYp3XPxmY+9qjGWXj4GoLi3w/bhvTyszYxhYWoMC1MjKEyMYCyXaVex0mgEqAUBBSo18h4u6ZuZr8L9B4XIyn/ynYdbNrZEZ9eG8GneED1a2qNZI/299wSR1BUWFiIyMhLBwcHabXK5HP369cOJEyfKPebEiROl7lsEAP7+/ti9e3eF51EqlVAq/72UseR+HiqVSqe7kR+7fh97LyTg3j05juy8WKo3UJ9oNBrmIAGRtzNwMy0PgAzPtbTDwsBOEDRqqDRqsUPTScn/Q7r8vyRFhpBHdXLQ5RgWFiQqmUwGJ1szONmawa9l6ecEQUB6biHuPVyt6l5mPhIyC5CcXYDErHzcTs5AnsYI+ariGxGm5iiRWsl7eVTE1FgOlwbmcG5ojuaNLNHG0QqtHa3RzskGthaG2XxOJEVpaWlQq9XahTxKODo6IiYmptxjkpKSyt0/KSmpwvOEhIRgzpw5ZbYfOHAAFhaV//IgIlGGXbeMAMiBlMRKHydNzEEKrE0EvNpcgy6NUnDy74Nih1Mt4eHhYodQIwwhj6rkkJeXV+l9WViQZMlkMjSyUqCRlaLMTIdKpXp4s0J/FGpkyMgrnnHIylMhR1mE/EI1cguLUFik0d4ZHQCM5IBcJoOZiREsFUawMDWGjZkJGlubopGlArbmJuyPIKpHgoODS81yZGdnw9XVFQMGDICNjU2lX8flbhbcrqXi+vVraNWqNYz09JtytUbDHCTAUmGMQZ72OH0sAv3799fbG1iqVCqEh4frdQ6AYeRRnRxKZnIrg4UF6T1d7uVBRPrB3t4eRkZGSE5OLrU9OTkZTk5O5R7j5OSk0/4AoFAooFAoymzX9e7l3u726ORii335/2Bw31Z6/ccHc5CGkstPdP1vUYoMIQfAMPKoSg667K+fpTwRERk0U1NTeHt749ChQ9ptGo0Ghw4dgq+vb7nH+Pr6ltofKJ72r2h/IiKqWZyxICIiSZo6dSpGjx4NHx8fdOvWDUuWLEFubq52lahRo0bB2dkZISEhAIAPP/wQvXv3xsKFCzFkyBBs2bIFZ8+exdq1a8VMg4io3mBhQUREkjR8+HCkpqZi1qxZSEpKQufOnbF//35tg3Z8fHypVX/8/PywefNmfPbZZ5g5cyZat26N3bt38x4WRER1hIUFERFJ1uTJkzF58uRyn4uIiCizLTAwEIGBgbUcFRERlYc9FkREREREVG0sLIiIiIiIqNrq3aVQglB8PwNd1uQtoVKpkJeXh+zsbL1ebswQ8mAO0mEIeRhCDkD18ij5TCz5jKyv6vsYwRykwxDyMIQcAMPIo67Gh3pXWOTk5AAAXF1dRY6EiEh6cnJyYGtrK3YYouEYQURUvsqMDzKhnn09pdFokJCQAGtra8hkut1hueSOrHfu3NHpjqxSYwh5MAfpMIQ8DCEHoHp5CIKAnJwcNG3atNRKS/VNfR8jmIN0GEIehpADYBh51NX4UO9mLORyOVxcXKr1GjY2Nnr7H9ajDCEP5iAdhpCHIeQAVD2P+jxTUYJjRDHmIB2GkIch5AAYRh61PT7U36+liIiIiIioxrCwICIiIiKiamNhoQOFQoHZs2dDoVCIHUq1GEIezEE6DCEPQ8gBMJw89JUh/P6Zg3QYQh6GkANgGHnUVQ71rnmbiIiIiIhqHmcsiIiIiIio2lhYEBERERFRtbGwICIiIiKiamNhUUUvvfQSmjVrBjMzMzRp0gQjR45EQkKC2GHp5NatW3jnnXfg7u4Oc3NztGzZErNnz0ZhYaHYoenkyy+/hJ+fHywsLNCgQQOxw6m0FStWoHnz5jAzM0P37t1x+vRpsUPSyZEjR/Diiy+iadOmkMlk2L17t9gh6SwkJATPPPMMrK2t4eDggICAAMTGxoodlk5WrVqFTp06adcm9/X1xR9//CF2WPWevo8RhjI+APo5RnB8EJ8hjA9A3Y8RLCyqqG/fvti2bRtiY2Pxyy+/4MaNGxg2bJjYYekkJiYGGo0Ga9asweXLl7F48WKsXr0aM2fOFDs0nRQWFiIwMBCTJk0SO5RK27p1K6ZOnYrZs2cjKioKXl5e8Pf3R0pKitihVVpubi68vLywYsUKsUOpsr///htBQUE4efIkwsPDoVKpMGDAAOTm5oodWqW5uLjgq6++QmRkJM6ePYvnn38eL7/8Mi5fvix2aPWavo8RhjI+APo3RnB8kAZDGB8AEcYIgWrEnj17BJlMJhQWFoodSrV88803gru7u9hhVMmGDRsEW1tbscOolG7duglBQUHax2q1WmjatKkQEhIiYlRVB0DYtWuX2GFUW0pKigBA+Pvvv8UOpVoaNmworFu3Tuww6BGGMEbo8/ggCPozRnB8kCZDGR8EoXbHCM5Y1ID09HRs2rQJfn5+MDExETucasnKyoKdnZ3YYRi0wsJCREZGol+/ftptcrkc/fr1w4kTJ0SMjLKysgBAb/8fUKvV2LJlC3Jzc+Hr6yt2OPSQoYwRHB9qH8cH6dL38QGomzGChUU1fPrpp7C0tESjRo0QHx+PPXv2iB1StVy/fh3Lli3De++9J3YoBi0tLQ1qtRqOjo6ltjs6OiIpKUmkqEij0WDKlCno0aMHOnToIHY4Orl48SKsrKygUCgwceJE7Nq1C56enmKHVe8Z0hjB8aFucHyQJn0eH4C6HSNYWDxixowZkMlkT/yJiYnR7j99+nScO3cOBw4cgJGREUaNGgVBAvcb1DUPALh37x4GDhyIwMBATJgwQaTI/1WVHIiqIygoCJcuXcKWLVvEDkVnHh4eiI6OxqlTpzBp0iSMHj0aV65cETssg2MIY4QhjA8AxwiqW/o8PgB1O0bwztuPSE1Nxf3795+4T4sWLWBqalpm+927d+Hq6orjx4+LfgmCrnkkJCSgT58+ePbZZxEaGgq5XPx6syrvRWhoKKZMmYLMzMxajq56CgsLYWFhgR07diAgIEC7ffTo0cjMzNTLbzVlMhl27dpVKh99MnnyZOzZswdHjhyBu7u72OFUW79+/dCyZUusWbNG7FAMiiGMEYYwPgCGO0ZwfJAeQxsfgNodI4xr/BX1WOPGjdG4ceMqHavRaAAASqWyJkOqEl3yuHfvHvr27Qtvb29s2LBBMoNGdd4LqTM1NYW3tzcOHTqk/aDVaDQ4dOgQJk+eLG5w9YwgCPjggw+wa9cuREREGMygodFoJPFZZGgMYYwwhPEBMNwxguODdBjq+ADU7hjBwqIKTp06hTNnzuC5555Dw4YNcePGDXz++edo2bKl6LMVurh37x769OkDNzc3fPvtt0hNTdU+5+TkJGJkuomPj0d6ejri4+OhVqsRHR0NAGjVqhWsrKzEDa4CU6dOxejRo+Hj44Nu3bphyZIlyM3NxdixY8UOrdIePHiA69evax/HxcUhOjoadnZ2aNasmYiRVV5QUBA2b96MPXv2wNraWnsNs62tLczNzUWOrnKCg4MxaNAgNGvWDDk5Odi8eTMiIiLw559/ih1avWUIY4ShjA+A/o0RHB+kwRDGB0CEMaJW1poycBcuXBD69u0r2NnZCQqFQmjevLkwceJE4e7du2KHppMNGzYIAMr90SejR48uN4fDhw+LHdoTLVu2TGjWrJlgamoqdOvWTTh58qTYIenk8OHD5f7eR48eLXZolVbRf/8bNmwQO7RKGzdunODm5iaYmpoKjRs3Fl544QXhwIEDYodVrxnCGGEo44Mg6OcYwfFBfIYwPghC3Y8R7LEgIiIiIqJqk84Fk0REREREpLdYWBARERERUbWxsCAiIiIiompjYUFERERERNXGwoKIiIiIiKqNhQUREREREVUbCwsiIiIiIqo2FhZERERERFRtLCyIiIiIiKjaWFgQEREREVG1sbAgIiIiIqJqY2FBVMdSU1Ph5OSE+fPna7cdP34cpqamOHTokIiRERGRmDg+kL6TCYIgiB0EUX2zb98+BAQE4Pjx4/Dw8EDnzp3x8ssvY9GiRWKHRkREIuL4QPqMhQWRSIKCgnDw4EH4+Pjg4sWLOHPmDBQKhdhhERGRyDg+kL5iYUEkkvz8fHTo0AF37txBZGQkOnbsKHZIREQkARwfSF+xx4JIJDdu3EBCQgI0Gg1u3boldjhERCQRHB9IX3HGgkgEhYWF6NatGzp37gwPDw8sWbIEFy9ehIODg9ihERGRiDg+kD5jYUEkgunTp2PHjh04f/48rKys0Lt3b9ja2mLv3r1ih0ZERCLi+ED6jJdCEdWxiIgILFmyBGFhYbCxsYFcLkdYWBiOHj2KVatWiR0eERGJhOMD6TvOWBARERERUbVxxoKIiIiIiKqNhQUREREREVUbCwsiIiIiIqo2FhZERERERFRtLCyIiIiIiKjaWFgQEREREVG1sbAgIiIiIqJqY2FBRERERETVxsKCiIiIiIiqjYUFERERERFVGwsLIiIiIiKqNhYWRERERERUbf8PSdjymPFKu9oAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    # 将张量转成列表，避免调用 .numpy()\n",
    "    plt.plot(x.tolist(), y.tolist())\n",
    "    plt.title(f\"{label} activation\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "# (批, 令牌数, 768)\n",
    "#       │  Linear 768→3072\n",
    "#       ▼\n",
    "# (批, 令牌数, 3072)\n",
    "#       │  GELU\n",
    "#       ▼\n",
    "# (批, 令牌数, 3072)\n",
    "#       │  Linear 3072→768\n",
    "#       ▼\n",
    "# (批, 令牌数, 768)\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcut connections\n",
    "将输入再次加到该层的输出，缓解梯度衰减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without shortcut:\n",
      "layers.0.0.weight's grad mean: 0.0002\n",
      "layers.1.0.weight's grad mean: 0.0001\n",
      "layers.2.0.weight's grad mean: 0.0007\n",
      "layers.3.0.weight's grad mean: 0.0014\n",
      "layers.4.0.weight's grad mean: 0.0050\n",
      "With shortcut:\n",
      "layers.0.0.weight's grad mean: 0.0014\n",
      "layers.1.0.weight's grad mean: 0.0048\n",
      "layers.2.0.weight's grad mean: 0.0041\n",
      "layers.3.0.weight's grad mean: 0.0059\n",
      "layers.4.0.weight's grad mean: 0.0327\n"
     ]
    }
   ],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_size, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        # five \"Linear -> GELU\"\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_size[0], layer_size[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[1], layer_size[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[2], layer_size[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[3], layer_size[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[4], layer_size[5]), GELU()),\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()(output, target)\n",
    "    loss.backward()\n",
    "    for name, param in model.named_parameters():\n",
    "        # print(name) #`layers.0.0.bias` and `layers.1.0.weight`\n",
    "        # print(param.shape)\n",
    "        if \"weight\" in name:\n",
    "            g_mean = param.grad.abs().mean().item()\n",
    "            print(f\"{name}'s grad mean: {g_mean:.4f}\")\n",
    "\n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_no_short = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)\n",
    "print(\"Without shortcut:\")\n",
    "print_gradients(model_no_short, sample_input)\n",
    "\n",
    "model_no_short = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "print(\"With shortcut:\")\n",
    "print_gradients(model_no_short, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection attention and linear in a transformer block\n",
    "![connect_attention_linear](connect_attention_linear.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            drop_out=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        # Linear -> GELU -> Linear\n",
    "        self.ff = FeedForward(cfg)\n",
    "\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self attention\n",
    "        shortcut_x = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut_x\n",
    "\n",
    "        # feed forward\n",
    "        shortcut_x = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut_x\n",
    "        return x\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "out = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", out.shape)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT model\n",
    "\n",
    "![GPT](gpt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "torch.return_types.max(\n",
      "values=tensor([[2.6121, 2.5646, 2.3975, 2.3694],\n",
      "        [2.3718, 2.9749, 2.7942, 2.4123]], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([[ 2215, 36230, 49103, 37532],\n",
      "        [40412, 30882, 46430, 29873]]))\n",
      "parameters: 163059793\n",
      "torch.Size([50257, 768])\n",
      "torch.Size([1024, 768])\n",
      "Number of trainable parameters: 124412160\n",
      "Total size of the model: 622.02 MB\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257, # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768, # Embedding dimension\n",
    "    \"n_heads\": 12, # Number of attention heads\n",
    "    \"n_layers\": 12, # Number of layers\n",
    "    \"drop_rate\": 0.1, # Dropout rate\n",
    "    \"qkv_bias\": False # Query-Key-Value bias\n",
    "}\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_out = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_layer_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.output_layer = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch, context_length = x.shape\n",
    "        token_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb(torch.arange(context_length))\n",
    "        \n",
    "        x = token_emb + pos_emb\n",
    "        x = self.drop_out(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_layer_norm(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out.max(dim=-1))\n",
    "total_params = sum([p.numel() for p in model.parameters()])\n",
    "print(\"parameters:\", total_params)\n",
    "print(model.tok_emb.weight.shape)\n",
    "print(model.pos_emb.weight.shape)\n",
    "\n",
    "total_params_gpt2 = total_params - sum([p.numel() for p in model.output_layer.parameters()])\n",
    "\n",
    "print(\"Number of trainable parameters:\", total_params_gpt2)\n",
    "\n",
    "total_size_bytes = total_params * 4        # 32-bit float ⇒ 4 B\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

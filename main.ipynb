{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "# fetch input text\n",
    "import urllib.request\n",
    "url = 'https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt'\n",
    "file_path = 'the-verdict.txt'\n",
    "# urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "# split text into tokens\n",
    "import re\n",
    "preprocessed = [item for item in re.split(\n",
    "    r'([,.:;?_!\"()\\']|--|\\s)', raw_text) if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n",
      "('His', 51)\n"
     ]
    }
   ],
   "source": [
    "# convert tokens into token IDs\n",
    "\n",
    "# create vocabulary\n",
    "all_words = sorted(list(set(preprocessed)))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 50:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {id: token for token, id in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = [token.strip() for token in re.split(\n",
    "            r'([,.:;?_!\"()\\']|--|\\s)', text) if token.strip()]\n",
    "        return [self.str_to_int[s] for s in preprocessed]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # \"Hello , world !\"  →  \"Hello, world!\"\n",
    "        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', \" \".join([self.int_to_str[id] for id in ids]))\n",
    "\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = 'I turned to Mrs. Gisburn, who had lingered to give a lump of sugar to her spaniel in the dining-room.'\n",
    "print(tokenizer.decode(tokenizer.encode(text)) == text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# add special token for vocabulary\n",
    "all_tokens = all_words + ['<|endoftext|>', '<|unk|>']\n",
    "vocab_size = len(all_tokens)\n",
    "print(vocab_size)\n",
    "\n",
    "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    if i > len(vocab) - 5:\n",
    "        print(item)\n",
    "\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {id: token for token, id in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = [token.strip() for token in re.split(\n",
    "            r'([,.:;?_!\"()\\']|--|\\s)', text) if token.strip()]\n",
    "        preprocessed = [\n",
    "            token if token in self.str_to_int else '<|unk|>' for token in preprocessed]\n",
    "        return [self.str_to_int[s] for s in preprocessed]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # \"Hello , world !\"  →  \"Hello, world!\"\n",
    "        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', \" \".join([self.int_to_str[id] for id in ids]))\n",
    "\n",
    "\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n",
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n",
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n",
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n",
      "\n",
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "# data sampling with sliding window\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "\n",
    "enc_sample = enc_text[50:]\n",
    "# context size determines how many tokens are included int the input\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")\n",
    "\n",
    "# iterator all target\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    target = enc_sample[i]\n",
    "    print(context, \"---->\", target)\n",
    "print()\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    target = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([target]))\n",
    "\n",
    "# data loader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        tokens_id = tokenizer.encode(text)\n",
    "        for i in range(0, len(tokens_id) - max_length, stride):\n",
    "            input_chunk = tokens_id[i:i+max_length]\n",
    "            output_chunk = tokens_id[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(output_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(text, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n",
      "\n",
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)\n",
    "print()\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"Targets:\\n\", targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n",
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# token embeddings example\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)\n",
    "print(embedding_layer(torch.tensor([3])))\n",
    "print(embedding_layer(input_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n",
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# encoding word position\n",
    "vocab_size = 50257\n",
    "embed_dim = 256\n",
    "# token embedding\n",
    "token_emb = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "# load batch data\n",
    "inputs, _ = next(iter(create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)))\n",
    "token_vecs = token_emb(inputs)\n",
    "print(token_vecs.shape)\n",
    "\n",
    "# position embedding, max_length is 4, so it has '4' position\n",
    "pos_emb = torch.nn.Embedding(4, embed_dim)\n",
    "pos_vecs = pos_emb(torch.arange(4))\n",
    "print(pos_vecs.shape)\n",
    "\n",
    "# gpt input\n",
    "input_embeds = token_vecs + pos_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "Attention weight: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n",
      "Attention weight: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n",
      "Context vec: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# self-attention example\n",
    "# 1.计算查询与每个输入之间的点积（注意力得分）；\n",
    "# 2.使用 softmax 将得分归一化为权重；\n",
    "# 3.按权重求输入向量的加权和，得到上下文向量。\n",
    "\n",
    "# this is the embedding inputs\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89],  # Your    (x¹)\n",
    "     [0.55, 0.87, 0.66],  # journey (x²)\n",
    "     [0.57, 0.85, 0.64],  # starts  (x³)\n",
    "     [0.22, 0.58, 0.33],  # with    (x⁴)\n",
    "     [0.77, 0.25, 0.10],  # one     (x⁵)\n",
    "     [0.05, 0.80, 0.55]]  # step    (x⁶)\n",
    ")\n",
    "# the first step is to calculate the **attention scores** between the query token and each input token\n",
    "# calculate the attention score of x2\n",
    "query = inputs[1]\n",
    "attention_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attention_scores_2[i] = torch.dot(query, x_i)\n",
    "print(attention_scores_2)\n",
    "# normalize the score\n",
    "attention_weights_2_tmp = attention_scores_2 / attention_scores_2.sum()\n",
    "print(\"Attention weight:\", attention_weights_2_tmp)\n",
    "print(\"Sum:\", attention_weights_2_tmp.sum())\n",
    "# use softmax to normalize\n",
    "atention_weights_2 = torch.softmax(attention_scores_2, dim=0)\n",
    "print(\"Attention weight:\", atention_weights_2)\n",
    "print(\"Sum:\", atention_weights_2.sum())\n",
    "# calculate the context vector\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += atention_weights_2[i] * x_i\n",
    "print(\"Context vec:\", context_vec_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n",
      "Previous 2nd context vec: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# calculate the attention weights for all input tokens\n",
    "attention_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attention_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attention_scores)\n",
    "\n",
    "attention_scores = inputs @ inputs.T\n",
    "print(attention_scores)\n",
    "# normalize the scores\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "print(attention_weights.sum(dim=-1))\n",
    "# context vec\n",
    "all_context_vecs = attention_weights @ inputs\n",
    "print(all_context_vecs)\n",
    "print(\"Previous 2nd context vec:\", context_vec_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## self-attention with trainable weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n",
      "torch.Size([3, 2])\n",
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "tensor(1.8524, grad_fn=<DotBackward0>)\n",
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "       grad_fn=<SqueezeBackward4>)\n",
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.3061, 0.8210], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# use Wq,Wk,Wv\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2  # for example\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out, requires_grad=False))\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out, requires_grad=False))\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)\n",
    "print(W_query.shape)\n",
    "# all inputs\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "# attention score for x2\n",
    "keys_2 = keys[1]\n",
    "attention_score22 = query_2.dot(keys_2)\n",
    "print(attention_score22)\n",
    "# all attention scores for give query 2\n",
    "attention_scores_2 = query_2 @ keys.T\n",
    "print(attention_scores_2)\n",
    "# scale\n",
    "d_k = keys.shape[-1]\n",
    "attention_weights_2 = torch.softmax(attention_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attention_weights_2)\n",
    "# context vec\n",
    "context_vec_2 = attention_weights_2 @ values\n",
    "print(context_vec_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vecs = attention_weights @ values\n",
    "        return context_vecs\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5337, -0.1051],\n",
      "        [-0.5323, -0.1080],\n",
      "        [-0.5323, -0.1079],\n",
      "        [-0.5297, -0.1076],\n",
      "        [-0.5311, -0.1066],\n",
      "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# use nn.Linear\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vecs = attention_weights @ values\n",
    "        return context_vecs\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hide future words with causal attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n",
      "        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n",
      "        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n",
      "        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n",
      "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[0.1717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1636, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1637, 0.1749, 0.1746, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1636, 0.1704, 0.1702, 0.1652, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.1855, 0.8812],\n",
      "        [0.2938, 0.9445],\n",
      "        [0.3258, 0.9576],\n",
      "        [0.3036, 0.8564],\n",
      "        [0.2737, 0.7564],\n",
      "        [0.2818, 0.7599]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 注意力矩阵主对角线之上的「未来词元」对应的权重全部遮蔽（置为 -∞），然后仅对未被遮蔽的权重做 softmax 归一化，使得每一行有效权重之和为 1\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attention_scores = queries @ keys.T\n",
    "attention_weights = torch.softmax(\n",
    "    attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attention_weights)\n",
    "# create a mask where the values above the diagonal are zero\n",
    "context_length = attention_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(\n",
    "    context_length, context_length), diagonal=0)\n",
    "print(mask_simple)\n",
    "mask_simple = attention_weights * mask_simple\n",
    "print(mask_simple)\n",
    "row_sums = mask_simple.sum(dim=-1, keepdim=True)\n",
    "mask_simple_normalize = mask_simple / row_sums\n",
    "# print(mask_simple_normalize)\n",
    "\n",
    "# another method to mask attention weights\n",
    "# 对角线以上全为1\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attention_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "# print(masked)\n",
    "attention_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attention_weights)\n",
    "context_vec = attention_weights @ values\n",
    "print(context_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n",
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.6816, 0.6804, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5085, 0.4936, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3906, 0.0000],\n",
      "        [0.3249, 0.3418, 0.0000, 0.3308, 0.3249, 0.3363]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "# Masking additional attention weights with dropout\n",
    "torch.manual_seed(123)\n",
    "drop_out = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(drop_out(example))\n",
    "print(drop_out(attention_weights))\n",
    "# test batch inputs\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, drop_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.triu(\n",
    "            torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, _ = x.shape\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # here we think about the batch dim: ->(b, d_out, num_tokens)\n",
    "        attention_scores = queries @ keys.transpose(1, 2)\n",
    "        attention_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        context_vecs = attention_weights @ values\n",
    "        return context_vecs\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, drop_out=0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape\", context_vecs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending single-head attention to multi-head attention\n",
    "\n",
    "### Stacking multiple single-head attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "# stack CausalAttention\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, drop_out, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CausalAttention(\n",
    "            d_in, d_out, context_length, drop_out, qkv_bias) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing multi-head attention with weight splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, drop_out, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisiable by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # why we need a output projection?\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.drop_out = nn.Dropout(drop_out)\n",
    "\n",
    "        self.register_buffer(\"mask\", torch.triu(\n",
    "            torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x:(batch, num_tokens, d_in)\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # q:(batch, num_tokens, d_out)\n",
    "        q = self.W_query(x)\n",
    "        k = self.W_key(x)\n",
    "        v = self.W_value(x)\n",
    "\n",
    "        # view:(batch, heads, num_tokens, head_dim)\n",
    "        q = q.view(b, num_tokens, self.num_heads,\n",
    "                   self.head_dim).transpose(1, 2)\n",
    "        k = k.view(b, num_tokens, self.num_heads,\n",
    "                   self.head_dim).transpose(1, 2)\n",
    "        v = v.view(b, num_tokens, self.num_heads,\n",
    "                   self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # attention_scores:(batch, heads, num_tokens, num_tokens)\n",
    "        attention_scores = q @ k.transpose(2, 3)\n",
    "        attention_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "\n",
    "        attention_weights = torch.softmax(\n",
    "            attention_scores / k.shape[-1]**0.5, dim=-1)\n",
    "        attention_weights = self.drop_out(attention_weights)\n",
    "\n",
    "        # context_vecs:(batch, num_tokens, heads, head_dim)\n",
    "        context_vecs = (attention_weights @ v).transpose(1, 2)\n",
    "        context_vecs = context_vecs.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vecs = self.out_proj(context_vecs)\n",
    "        return context_vecs\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_size, context_len, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_len, drop_out=0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(context_vecs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT\n",
    "## LLM architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,  # Embedding dimension\n",
    "    \"n_heads\": 12,  # Number of attention heads\n",
    "    \"n_layers\": 12,  # Number of layers\n",
    "    \"drop_rate\": 0.1,  # Dropout rate\n",
    "    \"qkv_bias\": False  # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLinearNorm(nn.Module):\n",
    "    def __init__(self, normalized_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(cfg[\"vocab_sizes\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emd_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = DummyLinearNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        token_emb = self.token_emb(in_idx)\n",
    "        pos_emb = self.pos_emb(torch.arange(seq_len))\n",
    "        x = token_emb + pos_emb\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch = torch.stack([torch.tensor(tokenizer.encode(txt1)),\n",
    "                    torch.tensor(tokenizer.encode(txt2))], dim=0)\n",
    "print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "tensor(-4.7684e-08, grad_fn=<MeanBackward0>) tensor(1.1111, grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Layer Normalization\n",
    "\n",
    "torch.manual_seed(123)\n",
    "example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(example)\n",
    "# print(out)\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "# print(mean)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "# print(var)\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "print(out_norm)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "# print(mean)\n",
    "# print(var)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        return self.scale * (x - mean) / (torch.sqrt(var + self.eps)) + self.shift\n",
    "\n",
    "\n",
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(example)\n",
    "print(out_ln.mean(), out_ln.var())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward network with GELU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mGELU\u001b[39;00m(nn.Module):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    # 将张量转成列表，避免调用 .numpy()\n",
    "    plt.plot(x.tolist(), y.tolist())\n",
    "    plt.title(f\"{label} activation\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "# (批, 令牌数, 768)\n",
    "#       │  Linear 768→3072\n",
    "#       ▼\n",
    "# (批, 令牌数, 3072)\n",
    "#       │  GELU\n",
    "#       ▼\n",
    "# (批, 令牌数, 3072)\n",
    "#       │  Linear 3072→768\n",
    "#       ▼\n",
    "# (批, 令牌数, 768)\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcut connections\n",
    "将输入再次加到该层的输出，缓解梯度衰减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without shortcut:\n",
      "layers.0.0.weight's grad mean: 0.0002\n",
      "layers.1.0.weight's grad mean: 0.0001\n",
      "layers.2.0.weight's grad mean: 0.0007\n",
      "layers.3.0.weight's grad mean: 0.0014\n",
      "layers.4.0.weight's grad mean: 0.0050\n",
      "With shortcut:\n",
      "layers.0.0.weight's grad mean: 0.0014\n",
      "layers.1.0.weight's grad mean: 0.0048\n",
      "layers.2.0.weight's grad mean: 0.0041\n",
      "layers.3.0.weight's grad mean: 0.0059\n",
      "layers.4.0.weight's grad mean: 0.0327\n"
     ]
    }
   ],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_size, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        # five \"Linear -> GELU\"\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_size[0], layer_size[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[1], layer_size[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[2], layer_size[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[3], layer_size[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[4], layer_size[5]), GELU()),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()(output, target)\n",
    "    loss.backward()\n",
    "    for name, param in model.named_parameters():\n",
    "        # print(name) #`layers.0.0.bias` and `layers.1.0.weight`\n",
    "        # print(param.shape)\n",
    "        if \"weight\" in name:\n",
    "            g_mean = param.grad.abs().mean().item()\n",
    "            print(f\"{name}'s grad mean: {g_mean:.4f}\")\n",
    "\n",
    "\n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_no_short = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)\n",
    "print(\"Without shortcut:\")\n",
    "print_gradients(model_no_short, sample_input)\n",
    "\n",
    "model_no_short = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "print(\"With shortcut:\")\n",
    "print_gradients(model_no_short, sample_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection attention and linear in a transformer block\n",
    "![connect_attention_linear](img/connect_attention_linear.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FeedForward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m torch.manual_seed(\u001b[32m123\u001b[39m)\n\u001b[32m     38\u001b[39m x = torch.rand(\u001b[32m2\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m768\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m block = \u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGPT_CONFIG_124M\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m out = block(x)\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInput shape:\u001b[39m\u001b[33m\"\u001b[39m, x.shape)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mTransformerBlock.__init__\u001b[39m\u001b[34m(self, cfg)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mself\u001b[39m.att = MultiHeadAttention(\n\u001b[32m      5\u001b[39m     d_in=cfg[\u001b[33m\"\u001b[39m\u001b[33memb_dim\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      6\u001b[39m     d_out=cfg[\u001b[33m\"\u001b[39m\u001b[33memb_dim\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     qkv_bias=cfg[\u001b[33m\"\u001b[39m\u001b[33mqkv_bias\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     11\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Linear -> GELU -> Linear\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28mself\u001b[39m.ff = \u001b[43mFeedForward\u001b[49m(cfg)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mself\u001b[39m.norm1 = LayerNorm(cfg[\u001b[33m\"\u001b[39m\u001b[33memb_dim\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     16\u001b[39m \u001b[38;5;28mself\u001b[39m.norm2 = LayerNorm(cfg[\u001b[33m\"\u001b[39m\u001b[33memb_dim\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'FeedForward' is not defined"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            drop_out=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        # Linear -> GELU -> Linear\n",
    "        self.ff = FeedForward(cfg)\n",
    "\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self attention\n",
    "        shortcut_x = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut_x\n",
    "\n",
    "        # feed forward\n",
    "        shortcut_x = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut_x\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "out = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT model\n",
    "\n",
    "![GPT](img/gpt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "torch.return_types.max(\n",
      "values=tensor([[2.6121, 2.5646, 2.3975, 2.3694],\n",
      "        [2.3718, 2.9749, 2.7942, 2.4123]], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([[ 2215, 36230, 49103, 37532],\n",
      "        [40412, 30882, 46430, 29873]]))\n",
      "parameters: 163059793\n",
      "torch.Size([50257, 768])\n",
      "torch.Size([1024, 768])\n",
      "Number of trainable parameters: 124412160\n",
      "Total size of the model: 622.02 MB\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,  # Embedding dimension\n",
    "    \"n_heads\": 12,  # Number of attention heads\n",
    "    \"n_layers\": 12,  # Number of layers\n",
    "    \"drop_rate\": 0.1,  # Dropout rate\n",
    "    \"qkv_bias\": False  # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_out = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_layer_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.output_layer = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, context_length = x.shape\n",
    "        token_emb = self.tok_emb(x)\n",
    "        pos_ids = torch.arange(context_length, device=x.device)\n",
    "        pos_emb = self.pos_emb(pos_ids)\n",
    "\n",
    "        x = token_emb + pos_emb\n",
    "        x = self.drop_out(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_layer_norm(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out.max(dim=-1))\n",
    "total_params = sum([p.numel() for p in model.parameters()])\n",
    "print(\"parameters:\", total_params)\n",
    "print(model.tok_emb.weight.shape)\n",
    "print(model.pos_emb.weight.shape)\n",
    "\n",
    "total_params_gpt2 = total_params - \\\n",
    "    sum([p.numel() for p in model.output_layer.parameters()])\n",
    "\n",
    "print(\"Number of trainable parameters:\", total_params_gpt2)\n",
    "\n",
    "total_size_bytes = total_params * 4        # 32-bit float ⇒ 4 B\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text\n",
    "\n",
    "![generation_mechanics](img/generation_mechanics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 仅保留最近 context_size 个 token 作为上下文，防止超过最大上下文长度\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "            # 每个batch的最后一行 (batch,vocab_size)\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![iteration](img/iteration.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape\", encoded_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length, 10\n",
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "out = generate_text_simple(model=model, idx=encoded_tensor,\n",
    "                           max_new_tokens=6, context_size=GPT_CONFIG_124M[\"context_length\"])\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length,\", len(out[0]))\n",
    "docoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(docoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining on unlabeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      " Every effort moves you rentingetic chief refusing holidays Shannon GamergateHay men methamphetamine\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,   # 将上下文长度缩短至 256\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(model=model, idx=text_to_token_ids(\n",
    "    start_context, tokenizer), max_new_tokens=10, context_size=GPT_CONFIG_124M[\"context_length\"])\n",
    "print(\"Result:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the text generation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n",
      "Token IDs:\n",
      " tensor([[[50153],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n",
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  PRESIDENT heNetflix\n",
      "Text 1: tensor([7.6198e-05, 3.1919e-05, 1.1728e-05])\n",
      "Text 2: tensor([1.0538e-05, 5.5378e-05, 4.9063e-06])\n",
      "log_probas: tensor([ -9.4822, -10.3523, -11.3535, -11.4605,  -9.8013, -12.2250])\n",
      "tensor(-10.7791)\n",
      "tensor(10.7791)\n",
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n",
      "tensor(10.7791)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([\n",
    "    [16833,  3626,  6100],   # \"every effort moves\"\n",
    "    [40,  1107,   588]    # \"I really like\"\n",
    "])\n",
    "targets = torch.tensor([\n",
    "    [3626,  6100,   345],   # \" effort moves you\"\n",
    "    [1107,   588, 11311]    # \" really like chocolate\"\n",
    "])\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)  # [2, 3, 50257]\n",
    "    probas = torch.softmax(logits, dim=-1)\n",
    "    print(probas.shape)\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(\n",
    "    f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n",
    "\n",
    "# 目标 token 对应的初始 softmax 概率分数,我们的目的是最大化target_probas\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)\n",
    "\n",
    "# loss 在数学优化中，直接处理概率分数不如处理其对数方便\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(\"log_probas:\", log_probas)\n",
    "# 训练的目标是通过更新模型权重，使平均对数概率尽可能逼近 0。不过在深度学习实践中，我们通常不是直接把平均对数概率推高到 0，而是最小化其相反数（负平均对数概率）\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)\n",
    "# 将 −10.7940 取负得到 10.7940 的做法被称为“交叉熵损失”（cross entropy loss）\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)  # [2, 3, 50257]\n",
    "print(\"Targets shape:\", targets.shape)  # [2, 3]\n",
    "\n",
    "# flatten in batch dim for cross_entropy\n",
    "logits_flat = logits.flatten(0, 1)  # [batch * seq_len, vocab_size]\n",
    "targets_flat = targets.flatten()\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the training and validation set losses\n",
    "\n",
    "![prepare_data](img/prepare_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n",
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)\n",
    "\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.990697224934896\n",
      "Validation loss: 10.985529899597168\n"
     ]
    }
   ],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    # (batch*seq_len, vocab_size) (batch*seq_len)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches == None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\",   train_loss)\n",
    "print(\"Validation loss:\", val_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "![training_iteration](img/training_iteration.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20, step 000179 train_loss4.839, val_loss6.967: 100%|██████████| 20/20 [00:22<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "\n",
    "def evalute_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        # 在start_context后面输出50个token\n",
    "        token_ids = generate_text_simple(\n",
    "            model, idx=encoded, max_new_tokens=100, context_size=context_size)\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def train_model_sample(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    token_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in (t := trange(num_epochs)):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            token_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evalute_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(token_seen)\n",
    "\n",
    "            t.set_description(\n",
    "                f\"Ep {epoch + 1}, step {global_step:06d} train_loss{train_loss:.3f}, val_loss{val_loss:.3f}\")\n",
    "\n",
    "        # generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses, val_losses, tokens_seen = train_model_sample(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq=5, eval_iter=5,\n",
    "                                                           start_context=\"Every effort moves you\", tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you---- of the of the of the                       of the I had, and the first.     of thehum.                                                     \n"
     ]
    }
   ],
   "source": [
    "generate_and_print_sample(model, tokenizer, device, \"Every effort moves you\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Train loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Val loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "epochs_tensor = torch.linspace(\n",
    "    0, num_epochs, len(train_losses))  # 生成均匀分布的 epoch 值\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you---- of the of the of the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(model, idx=text_to_token_ids(\n",
    "    \"Every effort moves you\", tokenizer), max_new_tokens=25, context_size=GPT_CONFIG_124M[\"context_length\"])\n",
    "print(token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smooth/fun/llm-from-scratch/.venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:4.133336544036865\n",
      "unembedding_out grad sum:  tensor(7.1540e-06)\n",
      "ln_f_out grad sum:  tensor(-0.0403)\n",
      "wte.weight grad sum:  tensor(0.0011)\n",
      "ln_f_weight grad sum:  tensor(-0.5387)\n",
      "res_2_out grad sum:  tensor(-1.8626e-09)\n",
      "h.11.mlp.c_proj.bias tensor(0.4879)\n",
      "h.11.mlp.c_proj.weight tensor(348.2757)\n",
      "mlp_gelu_out grad sum:  tensor(58.9445)\n",
      "mlp_c_fc_out grad sum:  tensor(14.5792)\n",
      "h.11.mlp.c_fc.bias sum:  tensor(3.6673)\n",
      "h.11.mlp.c_fc.weight sum:  tensor(523.4512)\n",
      "h.11.ln_2.bias sum:  tensor(11.7334)\n",
      "h.11.ln_2.weight sum:  tensor(5.9324)\n",
      "h.11.attn.c_proj.weight sum:  tensor(179.9422)\n",
      "h.11.attn.c_proj.bias sum:  tensor(0.6211)\n",
      "attn_z_out sum:  tensor(10.8570)\n",
      "wte.weight grad sum:  tensor(922.3688)\n"
     ]
    }
   ],
   "source": [
    "import safetensors\n",
    "import struct\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "f = open(\"data\", \"rb\")\n",
    "token_bytes = f.read(65 * 2)\n",
    "tokens = struct.unpack(\"65H\", token_bytes)\n",
    "# print(tokens)\n",
    "# print(len(tokens))\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt2\")\n",
    "# print(enc.decode(list(tokens[:65])))\n",
    "\n",
    "f = open(\"model.safetensors\", \"rb\")\n",
    "fts = safetensors.deserialize(f.read())\n",
    "params = dict()\n",
    "for ft in fts:\n",
    "    name = ft[0]\n",
    "    data = ft[1][\"data\"]\n",
    "    shape = ft[1][\"shape\"]\n",
    "    params[name] = torch.frombuffer(\n",
    "        data, dtype=torch.float32).reshape(shape).requires_grad_(True)\n",
    "    params[name].retain_grad()\n",
    "\n",
    "x = torch.tensor(tokens[:64], dtype=torch.long)\n",
    "y_true = torch.tensor(tokens[1:65], dtype=torch.long)\n",
    "\n",
    "input_size = len(x)\n",
    "d_model = 768\n",
    "d_k = 64\n",
    "\n",
    "wte_out = F.embedding(\n",
    "    input=x,\n",
    "    weight=params[\"wte.weight\"])\n",
    "wpe_out = F.embedding(\n",
    "    input=torch.arange(0, len(x), dtype=torch.long),\n",
    "    weight=params[\"wpe.weight\"])\n",
    "embedding_out = wte_out + wpe_out\n",
    "# print(embedding_out.sum())\n",
    "for layer_i in range(12):\n",
    "    layer_in = embedding_out if layer_i == 0 else res_2_out\n",
    "    ln_1_out = F.layer_norm(\n",
    "        input=layer_in,\n",
    "        normalized_shape=[d_model],\n",
    "        weight=params[f\"h.{layer_i}.ln_1.weight\"],\n",
    "        bias=params[f\"h.{layer_i}.ln_1.bias\"],\n",
    "        eps=1e-5)\n",
    "    # print(ln_1_out.sum())\n",
    "    attn_c_attn_out = F.linear(\n",
    "        input=ln_1_out,\n",
    "        weight=params[f\"h.{layer_i}.attn.c_attn.weight\"].transpose(0, 1),\n",
    "        bias=params[f\"h.{layer_i}.attn.c_attn.bias\"])\n",
    "    # print(attn_c_attn_out.sum())\n",
    "    q, k, v = attn_c_attn_out.split(d_model, dim=1)\n",
    "    attn_z_out = torch.zeros([input_size, d_model])\n",
    "    for head_i in range(12):\n",
    "        a = q[:, head_i*d_k:(head_i+1)*d_k] @ k[:, head_i*d_k:(head_i+1)*d_k].transpose(\n",
    "            0, 1) * torch.rsqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "        mask = torch.triu(torch.ones_like(a, dtype=torch.bool), diagonal=1)\n",
    "        a = torch.masked_fill(a, mask, -torch.inf)\n",
    "        s = F.softmax(a, dim=-1)\n",
    "\n",
    "        z = s @ v[:, head_i*d_k:(head_i+1)*d_k]\n",
    "        attn_z_out[:, head_i*d_k:(head_i+1)*d_k] = z\n",
    "    attn_z_out.retain_grad()\n",
    "\n",
    "    # print(attn_z_out.sum())\n",
    "    # exit()\n",
    "    attn_c_proj_out = F.linear(\n",
    "        input=attn_z_out,\n",
    "        weight=params[f\"h.{layer_i}.attn.c_proj.weight\"].transpose(0, 1),\n",
    "        bias=params[f\"h.{layer_i}.attn.c_proj.bias\"])\n",
    "    # print(attn_c_proj_out.sum())\n",
    "    # exit()\n",
    "    res_1_out = layer_in + attn_c_proj_out\n",
    "    # print(res_1_out.sum())\n",
    "    # exit()\n",
    "    ln_2_out = F.layer_norm(\n",
    "        input=res_1_out,\n",
    "        normalized_shape=[d_model],\n",
    "        weight=params[f\"h.{layer_i}.ln_2.weight\"],\n",
    "        bias=params[f\"h.{layer_i}.ln_2.bias\"],\n",
    "        eps=1e-5)\n",
    "    # print(\"ln_2_out:\", ln_2_out.sum())\n",
    "    # exit()\n",
    "    mlp_c_fc_out = F.linear(\n",
    "        input=ln_2_out,\n",
    "        weight=params[f\"h.{layer_i}.mlp.c_fc.weight\"].transpose(0, 1),\n",
    "        bias=params[f\"h.{layer_i}.mlp.c_fc.bias\"])\n",
    "\n",
    "    mlp_c_fc_out.retain_grad()\n",
    "    # print(\"mlp_c_fc_out:\", mlp_c_fc_out.sum(), \"shape\", mlp_c_fc_out.shape)\n",
    "    # exit()\n",
    "    mlp_gelu_out = F.gelu(mlp_c_fc_out)\n",
    "    # print(mlp_gelu_out.sum())\n",
    "    # exit()\n",
    "    mlp_c_proj_out = F.linear(\n",
    "        input=mlp_gelu_out,\n",
    "        weight=params[f\"h.{layer_i}.mlp.c_proj.weight\"].transpose(0, 1),\n",
    "        bias=params[f\"h.{layer_i}.mlp.c_proj.bias\"])\n",
    "    # print(mlp_c_proj_out.sum())\n",
    "    # exit()\n",
    "    res_2_out = res_1_out + mlp_c_proj_out\n",
    "    # print(res_2_out.sum())\n",
    "    # exit()\n",
    "    # print(res_2_out.shape)\n",
    "    mlp_gelu_out.retain_grad()\n",
    "\n",
    "ln_f_out = F.layer_norm(\n",
    "    input=res_2_out,\n",
    "    normalized_shape=[d_model],\n",
    "    weight=params[\"ln_f.weight\"],\n",
    "    bias=params[\"ln_f.bias\"],\n",
    "    eps=1e-5)\n",
    "\n",
    "\n",
    "unembedding_out = F.linear(\n",
    "    input=ln_f_out,\n",
    "    weight=params[\"wte.weight\"])\n",
    "ln_f_out.retain_grad()\n",
    "# print(unembedding_out.sum(), unembedding_out.shape)\n",
    "# exit()\n",
    "\n",
    "\n",
    "unembedding_out.retain_grad()\n",
    "\n",
    "target_token_id = torch.argmax(unembedding_out[-1, :], dim=-1)\n",
    "# print(enc.decode(list(x)))\n",
    "# print(target_token_id)\n",
    "# word = enc.decode([target_token_id])\n",
    "# print(word)\n",
    "\n",
    "\n",
    "loss = F.cross_entropy(unembedding_out, y_true)\n",
    "print(f\"loss:{loss}\")\n",
    "# assert(abs(loss - 4.1333) < 1e-3)\n",
    "\n",
    "res_2_out.retain_grad()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"unembedding_out grad sum: \", unembedding_out.grad.sum())\n",
    "print(\"ln_f_out grad sum: \", ln_f_out.grad.sum())\n",
    "print(\"wte.weight grad sum: \", params[\"wte.weight\"].grad.sum())\n",
    "print(\"ln_f_weight grad sum: \", params[\"ln_f.weight\"].grad.sum())\n",
    "print(\"res_2_out grad sum: \", res_2_out.grad.sum())\n",
    "print(\"h.11.mlp.c_proj.bias\", torch.abs(\n",
    "    params[f\"h.11.mlp.c_proj.bias\"].grad).sum())\n",
    "print(\"h.11.mlp.c_proj.weight\", torch.abs(\n",
    "    params[f\"h.11.mlp.c_proj.weight\"].grad).sum())\n",
    "print(\"mlp_gelu_out grad sum: \", torch.abs(mlp_gelu_out.grad).sum())\n",
    "print(\"mlp_c_fc_out grad sum: \", torch.abs(mlp_c_fc_out.grad).sum())\n",
    "\n",
    "print(\"h.11.mlp.c_fc.bias sum: \", torch.abs(\n",
    "    params[f\"h.11.mlp.c_fc.bias\"].grad).sum())\n",
    "print(\"h.11.mlp.c_fc.weight sum: \", torch.abs(\n",
    "    params[f\"h.11.mlp.c_fc.weight\"].grad).sum())\n",
    "print(\"h.11.ln_2.bias sum: \", torch.abs(params[f\"h.11.ln_2.bias\"].grad).sum())\n",
    "print(\"h.11.ln_2.weight sum: \", torch.abs(\n",
    "    params[f\"h.11.ln_2.weight\"].grad).sum())\n",
    "print(\"h.11.attn.c_proj.weight sum: \", torch.abs(\n",
    "    params[f\"h.11.attn.c_proj.weight\"].grad).sum())\n",
    "print(\"h.11.attn.c_proj.bias sum: \", torch.abs(\n",
    "    params[f\"h.11.attn.c_proj.bias\"].grad).sum())\n",
    "print(\"attn_z_out sum: \", torch.abs(attn_z_out.grad).sum())\n",
    "\n",
    "print(\"wte.weight grad sum: \", torch.abs(\n",
    "    params[\"wte.weight\"].grad).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
